{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO RE-RUN\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"last\" # ['first','last']\n",
    "typeHypothesis = \"all_readmisssion_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended\" # [\"minimum,\"extended']\n",
    "typeDisease = \"Respiratory\" # [\"subset\",\"Respiratory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71518, 30)\n",
      "Index([u'diss_1', u'adm_src_ref', u'adm_src_em', u'race_AfricanAmerican',\n",
      "       u'race_Caucasian', u'race_Other', u'medSpec_cardio',\n",
      "       u'medSpec_Family/GeneralPractice', u'medSpec_InternalMedicine',\n",
      "       u'medSpec_surgery', u'age_cat', u'Diabetis', u'Circulatory',\n",
      "       u'Digestive', u'Genitourinary', u'Poisoning', u'Muscoskeletal',\n",
      "       u'Neoplasms', u'Respiratory', u'HbA1c', u'Change', u'time_in_hospital',\n",
      "       u'num_lab_procedures', u'num_procedures', u'num_medications',\n",
      "       u'number_outpatient', u'number_emergency', u'number_inpatient',\n",
      "       u'number_diagnoses', u'readmitted'],\n",
      "      dtype='object')\n",
      "0    42985\n",
      "2    22240\n",
      "1     6293\n",
      "Name: readmitted, dtype: int64\n",
      "0   0.60\n",
      "2   0.31\n",
      "1   0.09\n",
      "Name: readmitted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if typeDataFeatures == \"minimum\":\n",
    "    df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + '_hyp_1.pkl'))\n",
    "if typeDataFeatures == \"extended\":\n",
    "    df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + '_hyp_1_' + typeDataFeatures + '.pkl'))\n",
    "    \n",
    "print df_all.shape\n",
    "print df_all.columns\n",
    "print df_all.readmitted.value_counts()\n",
    "print df_all.readmitted.value_counts()/float(df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Diabetis', u'Circulatory', u'Digestive', u'Genitourinary', u'Poisoning', u'Muscoskeletal', u'Neoplasms', u'Respiratory']\n",
      "['diss_1', 'adm_src_ref', 'adm_src_em', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other', 'medSpec_cardio', 'medSpec_Family/GeneralPractice', 'medSpec_InternalMedicine', 'medSpec_surgery', 'age_cat', 'HbA1c', 'Change', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'readmitted']\n"
     ]
    }
   ],
   "source": [
    "colsDiseases = [u'Diabetis', u'Circulatory', u'Digestive', u'Genitourinary', u'Poisoning', u'Muscoskeletal',\n",
    "       u'Neoplasms', u'Respiratory']\n",
    "colsNonDiseases = [c for c in df_all.columns if c not in colsDiseases]\n",
    "\n",
    "print colsDiseases\n",
    "print colsNonDiseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] 42985 6293 22240\n"
     ]
    }
   ],
   "source": [
    "print df_all.loc[:,\"readmitted\"].sort_values().unique(), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 0), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 1), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_readmisssion_vs_none\n",
      "[0 1] 42985 28533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Readmitted none vs readmitted\n",
    "print typeHypothesis\n",
    "\n",
    "if typeHypothesis == \"all_readmisssion_vs_none\":\n",
    "    df_all[\"readmitted\"][df_all[\"readmitted\"].values > 0] = 1\n",
    "    print df_all.iloc[:,-1].sort_values().unique(), \\\n",
    "            np.sum(df_all[\"readmitted\"] == 0), \\\n",
    "            np.sum(df_all[\"readmitted\"] == 1)\n",
    "            \n",
    "# Readmitted none vs early readmitted            \n",
    "if typeHypothesis == \"early_readmission_vs_none\":\n",
    "    df_all= df_all[df_all[\"readmitted\"].isin([0,1])]\n",
    "    print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute partition (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df_all, ts_thr=0.30):\n",
    "    y = df_all.readmitted\n",
    "    y = y.values\n",
    "\n",
    "    X = df_all.iloc[:,:-1].values\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=ts_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_partition(X_train, y_train, tr_thr=0.10):\n",
    "    X_train_aux = []\n",
    "    y_train_aux = []\n",
    "    sss = StratifiedShuffleSplit(y_train, 1, test_size=1-tr_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train_aux = X_train[train_index]\n",
    "        y_train_aux = y_train[train_index]\n",
    "\n",
    "    return X_train_aux, y_train_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute type fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_type_features(df_all):\n",
    "    if typeDataFeatures == \"minimum\":\n",
    "        numCols = ['time_in_hospital']\n",
    "\n",
    "    if typeDataFeatures == \"extended\":\n",
    "        numCols = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', \n",
    "                'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "    catCols = []\n",
    "    cols = df_all.columns\n",
    "    reducedCols = cols[:-1]\n",
    "\n",
    "    for i in range(len(cols)-1):\n",
    "        if cols[i] not in numCols:\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "    catCols = np.array(catCols)\n",
    "    \n",
    "    return catCols, reducedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(typeDisease):\n",
    "    if typeDisease == \"subset\":\n",
    "        return \"subset\"\n",
    "    else:\n",
    "        if typeDisease in colsDiseases:\n",
    "            return [typeDisease]\n",
    "        else:\n",
    "            return colsDiseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms):\n",
    "    basePipeline = Pipeline([\n",
    "            (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "            (\"Scaler\", StandardScaler()),\n",
    "            (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "        ])\n",
    "\n",
    "    pipeline = [] \n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        for sm_type in sm_types:\n",
    "            for cls_method in cls_methods:\n",
    "                for lm in lms:\n",
    "                    if not (fs_method == \"rfe_rf_fs\" and cls_method == \"rf\"):\n",
    "                        params = {}   \n",
    "                        pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "                        if fs_method == \"combine_fs\":\n",
    "                            pipe.steps.insert(1,(fs_method, UnivCombineFilter(catCols,np.array(reducedCols))))\n",
    "                            params.update({fs_method + '__percentile':[5,10,20,30,40,50]})\n",
    "\n",
    "                        if fs_method == \"rfe_rf_fs\":\n",
    "                            pipe.steps.append((fs_method, RFE(estimator=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                                               n_estimators=100,\n",
    "                                                                                               random_state=33))))\n",
    "                            params.update({fs_method + '__step':[0.1]})\n",
    "                            params.update({fs_method + '__n_features_to_select':[\n",
    "                                                            int(len(reducedCols)*0.4), \n",
    "                                                            int(len(reducedCols)*0.6), \n",
    "                                                            int(len(reducedCols)*0.8)]})\n",
    "\n",
    "                        if fs_method == 'lasso_fs':\n",
    "                            pipe.steps.append((fs_method, SelectFromModel(\n",
    "                                        LogisticRegression(n_jobs=-1, penalty=\"l1\", dual=False, random_state=42))))\n",
    "                            params.update({fs_method + '__estimator__C': [0.001,0.01,0.1,1]})\n",
    "\n",
    "                        #Add classifiers\n",
    "                        if cls_method == \"knn\":\n",
    "                            pipe.steps.append((cls_method, KNeighborsClassifier()))\n",
    "                            params.update({'knn__n_neighbors':[1,3,5,7,9,11], 'knn__weights':['uniform', 'distance']})\n",
    "\n",
    "                        if cls_method == \"logReg\":\n",
    "                            pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "                            params.update({'logReg__C': [0.00001,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,15,30]})\n",
    "                            params.update({'logReg__class_weight': [None, 'balanced']})\n",
    "                            params.update({'logReg__penalty': ['l1', 'l2']})\n",
    "\n",
    "                        if cls_method == \"svmRBF\":\n",
    "                            pipe.steps.append((cls_method, SVC(random_state=42,probability=True)))\n",
    "                            params.update({'svmRBF__C': [0.01,0.1,0.5,1,5,10,30,50,100], \n",
    "                             'svmRBF__gamma' : [0.0001,0.001,0.01, 0.1,1,5]})\n",
    "                            params.update({'svmRBF__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,random_state=42)))\n",
    "                            params.update({'rf__n_estimators': [100,150,200,250,500], \n",
    "                                           'rf__criterion': ['entropy','gini'],\n",
    "                                           'rf__max_depth' : [None,4,6]})\n",
    "                            params.update({'rf__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"nb\":\n",
    "                             pipe.steps.append((cls_method, GaussianNB()))\n",
    "                             params.update({})\n",
    "                                               \n",
    "                        if cls_method == \"nn\":\n",
    "                            pipe.steps.append((cls_method, MLPClassifier(\n",
    "                                        activation='logistic',\n",
    "                                        solver='lbfgs', \n",
    "                                        hidden_layer_sizes=(5, 2), \n",
    "                                        random_state=13)))\n",
    "                            params.update({\n",
    "                                    'nn__alpha': [1e-5,0.00001,0.0001,0.001,0.01,0.1,1,3,5,10],\n",
    "                                    'nn__hidden_layer_sizes':[(30,),(50,),(70,),(100,),(150,),\n",
    "                                                              (30,30),(50,50),(70,70),(100,100),\n",
    "                                                              (30,30,30),(50,50,50),(70,70,70)\n",
    "                                                             ]\n",
    "                                          })\n",
    "\n",
    "                        #Add sampling\n",
    "                        pipe_imb = make_pipeline(*[p[1] for p in pipe.steps])\n",
    "                        stps = len(pipe_imb.steps)        \n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.remove(pipe_imb.steps[0])\n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.append(pipe.steps[s])\n",
    "\n",
    "                        if sm_type == \"after\":                    \n",
    "                            pipe_imb.steps.insert(stps - 1, \n",
    "                                                  (sm_method, SMOTE(ratio='auto', kind='regular', random_state=32)))\n",
    "                            params.update({sm_method + \"__k_neighbors\":[3,4,5]})\n",
    "\n",
    "\n",
    "                        pipeline.append([fs_method,sm_type,cls_method,lm,pipe_imb,params])\n",
    "    pipeline = pd.DataFrame(pipeline, columns=[\"fs\",\"sm\",\"cls\",\"metric\",\"pipe\",\"pipe_params\"])\n",
    "    pipeline.sort_values(\"fs\", inplace=True)\n",
    "    print pipeline.shape\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = [0.20,0.40,0.60]\n",
    "ts_thr = 0.30\n",
    "\n",
    "fs_methods = [\"none\"] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"rf\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\"]\n",
    "lms = [\"roc_auc\",\"recall\"] #[\"f1_weighted\",\"precision_weighted\"]\n",
    "sm_types = [\"none\",\"after\"] #[\"none\",\"after\"]\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n",
      "\n",
      "DataSet:\n",
      "**********\n",
      "**********\n",
      "DISEASE: Respiratory\n",
      "(71518, 23)\n",
      "ALL TRAIN: (6436, 22)\n",
      "TRAIN: [0's: 3868 1's: 2568 ]\n",
      "ALL TEST: (7152, 22)\n",
      "TEST: [0's: 4299 1's: 2853 ]\n",
      "\n",
      "Num experiment: 0 / 3\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: rf\n",
      "METRIC: roc_auc\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: roc_auc\n",
      "CV INNER selected params ['entropy', 6, None, 500]\n",
      "CV INNER score: 0.641102721568\n",
      "\n",
      "CV OUTER f1 score: 0.548  (+/-0.006)\n",
      "CV OUTER prec score: 0.621  (+/-0.009)\n",
      "CV OUTER rec score: 0.624  (+/-0.004)\n",
      "Selected params (bests from CV) ['entropy', 6, None, 500]\n",
      "\n",
      "TR F1 score: 0.601374032984\n",
      "TR Prec score: 0.705161975868\n",
      "TR Rec score: 0.664387818521\n",
      "\n",
      "Test f1: 0.561\n",
      "Test Precision: 0.630\n",
      "Test Recall: 0.630\n",
      "Test AUC: 0.553\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.93      0.75      4299\n",
      "          1       0.63      0.17      0.27      2853\n",
      "\n",
      "avg / total       0.63      0.63      0.56      7152\n",
      "\n",
      "[[4007  292]\n",
      " [2354  499]]\n",
      "\n",
      "Total time: 99.4810130596\n",
      "\n",
      "Num experiment: 1 / 3\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: rf\n",
      "METRIC: recall\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "subsetFeatures = get_features(typeDisease)\n",
    "\n",
    "for tr_thr in tr_thrs:\n",
    "    for disease in subsetFeatures:\n",
    "\n",
    "        if disease == \"subset\":\n",
    "            df_all_filtered = df_all.copy()\n",
    "        else:\n",
    "            cols_filtered = colsNonDiseases[:]\n",
    "            cols_filtered.insert(-1, disease)\n",
    "            df_all_filtered = df_all[cols_filtered].copy()\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_partition(df_all_filtered, ts_thr=0.10)\n",
    "        X_train, y_train = train_partition(X_train, y_train, tr_thr=0.10)\n",
    "\n",
    "        catCols, reducedCols = compute_type_features(df_all_filtered)\n",
    "        pipeline = create_pipeline(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms)\n",
    "\n",
    "        print \"\\nDataSet:\"\n",
    "        print \"**********\"\n",
    "        print \"**********\"\n",
    "        print \"DISEASE:\", disease\n",
    "\n",
    "        print df_all_filtered.shape\n",
    "        print \"ALL TRAIN:\", X_train.shape\n",
    "        print \"TRAIN:\", \"[0's:\", np.sum(y_train==0), \"1's:\", np.sum(y_train==1), \"]\"\n",
    "        print \"ALL TEST:\", X_test.shape\n",
    "        print \"TEST:\", \"[0's:\", np.sum(y_test==0), \"1's:\", np.sum(y_test==1), \"]\"\n",
    "\n",
    "        for num_exp in range(pipeline.shape[0]):\n",
    "\n",
    "            # Run experiment\n",
    "            start = time.time()\n",
    "\n",
    "            #Prepare pipe_cls      \n",
    "            pipeline_cls = pipeline[\"pipe\"].iloc[num_exp]\n",
    "            pipeline_params = pipeline[\"pipe_params\"].iloc[num_exp]\n",
    "            fs = pipeline[\"fs\"].iloc[num_exp]\n",
    "            sm = pipeline[\"sm\"].iloc[num_exp]\n",
    "            cls = pipeline[\"cls\"].iloc[num_exp]\n",
    "            lm = pipeline[\"metric\"].iloc[num_exp]\n",
    "\n",
    "            print \"\\nNum experiment:\", str(num_exp), \"/\", str(pipeline.shape[0] - 1)\n",
    "            print \"****************\"\n",
    "\n",
    "            print \"FS:\",fs\n",
    "            print \"SM:\",sm\n",
    "            print \"CLS:\",cls\n",
    "            print \"METRIC:\",lm\n",
    "\n",
    "            #Prepare cv\n",
    "            cv_inner = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=24)\n",
    "            cv_outer = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=42)\n",
    "\n",
    "            #Fit pipeline with CV                        \n",
    "            grid_pipeline = GridSearchCV(pipeline_cls, param_grid=pipeline_params, verbose=verbose, \n",
    "                                         n_jobs=-1, cv=cv_inner, scoring= lm, error_score = 0) \n",
    "            grid_pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Compute pipeline evaluation with CVmetric\n",
    "            print \"\\nCV INNER metric: {}\".format(lm)\n",
    "            print \"CV INNER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "            print \"CV INNER score: {}\".format(grid_pipeline.best_score_)\n",
    "\n",
    "            cv_f1 = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_prec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='precision_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_rec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "            print \"\\nCV OUTER f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_f1), np.std(cv_f1))\n",
    "            print \"CV OUTER prec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_prec), np.std(cv_prec))\n",
    "            print \"CV OUTER rec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_rec), np.std(cv_rec))\n",
    "            print \"Selected params (bests from CV) {}\".format(grid_pipeline.best_params_.values())\n",
    "\n",
    "            # Computel Train score (with best CV params)\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_train)\n",
    "            train_prec_scores = metrics.precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "            train_rec_scores = metrics.recall_score(y_train, y_pred, average='weighted', pos_label=None)    \n",
    "            train_f1_scores = metrics.f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "            print \"\\nTR F1 score:\", train_f1_scores\n",
    "            print \"TR Prec score:\", train_prec_scores\n",
    "            print \"TR Rec score:\", train_rec_scores\n",
    "\n",
    "            #Compute test score\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_test)\n",
    "            test_f1 = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_prec = metrics.recall_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_rec = metrics.precision_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_auc = metrics.roc_auc_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            print \"\\nTest f1: %0.3f\" % (test_f1)\n",
    "            print \"Test Precision: %0.3f\" % (test_prec)\n",
    "            print \"Test Recall: %0.3f\" % (test_rec)\n",
    "            print \"Test AUC: %0.3f\" % (test_auc)\n",
    "\n",
    "            print \"with following performance in test:\"\n",
    "            print metrics.classification_report(y_test, y_pred)\n",
    "            print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            end = time.time()\n",
    "            print \"\\nTotal time:\", end - start\n",
    "            results = [num_exp,\n",
    "                           disease,\n",
    "                           typeEncounter,\n",
    "                           typeHypothesis,\n",
    "                           typeDataFeatures, \n",
    "                           tr_thr,\n",
    "                           fs,\n",
    "                           sm,\n",
    "                           cls,\n",
    "                           lm,\n",
    "                           grid_pipeline.best_params_.values(),\n",
    "                           train_f1_scores,\n",
    "                           train_prec_scores,\n",
    "                           train_rec_scores,\n",
    "                           np.mean(cv_f1), \n",
    "                           np.std(cv_f1),\n",
    "                           np.mean(cv_prec), \n",
    "                           np.std(cv_prec),\n",
    "                           np.mean(cv_rec), \n",
    "                           np.std(cv_rec),                    \n",
    "                           test_f1,\n",
    "                           test_prec,\n",
    "                           test_rec,\n",
    "                           test_auc,                    \n",
    "                           end - start,\n",
    "                           grid_pipeline.best_estimator_\n",
    "                          ]\n",
    "\n",
    "            #Save results\n",
    "            df = pd.DataFrame(np.array(results).reshape(1,26), columns=\n",
    "                      [\"exp\", \"typeDisease\",\"typeEncounter\",\"typeHypothesis\",\"typeDataFeatures\",\n",
    "                       \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                       \"tr_f1\",\"tr_prec\",\"tr_rec\",\n",
    "                       \"cv_f1_mean\",\"cv_f1_std\",\"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                       \"test_f1\",\"test_prec\",\"test_rec\",\"test_auc\",\n",
    "                       \"time\",\"pipeline\"])\n",
    "\n",
    "            df.to_pickle(os.path.join(\"resources\", \"results\",\n",
    "                                      'results_pipe_' + \n",
    "                                      \"test_\" + str(ts_thr) + \"_\" +\n",
    "                                      \"train_\" + str(tr_thr) + \"_\" +\n",
    "                                      str(disease) + '_' +\n",
    "                                      str(typeEncounter) + '_' +\n",
    "                                      str(typeHypothesis) + '_' +\n",
    "                                      str(typeDataFeatures) + '_' +\n",
    "                                      str(fs) + '_' +\n",
    "                                      str(sm) + '_' +\n",
    "                                      str(lm) + '_' +\n",
    "                                      str(cls) + '_' +\n",
    "                                      '.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=\n",
    "                  [\"exp\",\n",
    "                   \"typeDisease\",\"typeEncounter\",\"typeHypothesis\",\"typeDataFeatures\",\n",
    "                   \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                   \"tr_f1\",\"tr_prec\",\"tr_rec\",\n",
    "                   \"cv_f1_mean\",\"cv_f1_std\",\"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                   \"test_f1\",\"test_prec\",\"test_rec\",\"test_auc\",\n",
    "                   \"time\",\"pipeline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(\"cv_f1_mean\", ascending=False).head(10).iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
