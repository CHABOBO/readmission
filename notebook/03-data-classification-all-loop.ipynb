{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO RE-RUN\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"last\" # ['first','last']\n",
    "typeHypothesis = \"all_readmisssion_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended\" # [\"minimum,\"extended']\n",
    "    #Extended -> Subset of columns\n",
    "    #Minimum -> minimum set of columns \n",
    "typeDisease = \"all\" # [\"subset\",\"all\",[\"Respiratory\",...]]\n",
    "    #All -> Return all disease features\n",
    "    #Subset -> Return subset of disease features\n",
    "    #Disase -> Return diases feature\n",
    "typeDataExperiment = \"disease\" #[\"all\", \"disease\"] \n",
    "        #All -> Include all diagnosis as columns\n",
    "        #Disease -> Remove diagnosis as column and keep only rows with diagnosis == 1\n",
    "        \n",
    "typeDiagnosis = \"diag_1\"  #[\"diag_1\", \"diag_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71518, 30)\n",
      "Index([u'diss_1', u'adm_src_ref', u'adm_src_em', u'race_AfricanAmerican',\n",
      "       u'race_Caucasian', u'race_Other', u'medSpec_cardio',\n",
      "       u'medSpec_Family/GeneralPractice', u'medSpec_InternalMedicine',\n",
      "       u'medSpec_surgery', u'age_cat', u'Diabetis_1', u'Circulatory_1',\n",
      "       u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
      "       u'Neoplasms_1', u'Respiratory_1', u'HbA1c', u'Change',\n",
      "       u'time_in_hospital', u'num_lab_procedures', u'num_procedures',\n",
      "       u'num_medications', u'number_outpatient', u'number_emergency',\n",
      "       u'number_inpatient', u'number_diagnoses', u'readmitted'],\n",
      "      dtype='object')\n",
      "0    42985\n",
      "2    22240\n",
      "1     6293\n",
      "Name: readmitted, dtype: int64\n",
      "0   0.60\n",
      "2   0.31\n",
      "1   0.09\n",
      "Name: readmitted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if typeDataFeatures == \"minimum\":\n",
    "    df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + \"_\" +  typeDiagnosis+ '_hyp_1.pkl'))\n",
    "if typeDataFeatures == \"extended\":\n",
    "    df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + \"_\" +  typeDiagnosis+ '_hyp_1_' + typeDataFeatures + '.pkl'))\n",
    "    \n",
    "print df_all.shape\n",
    "print df_all.columns\n",
    "print df_all.readmitted.value_counts()\n",
    "print df_all.readmitted.value_counts()/float(df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1', u'Neoplasms_1', u'Respiratory_1']\n",
      "['diss_1', 'adm_src_ref', 'adm_src_em', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other', 'medSpec_cardio', 'medSpec_Family/GeneralPractice', 'medSpec_InternalMedicine', 'medSpec_surgery', 'age_cat', 'HbA1c', 'Change', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'readmitted']\n"
     ]
    }
   ],
   "source": [
    "if typeDiagnosis == \"diag_1\":\n",
    "    colsDiseases = [u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
    "           u'Neoplasms_1', u'Respiratory_1']\n",
    "    \n",
    "if typeDiagnosis == \"diag_3\":\n",
    "    colsDiseases = [u'Diabetis_3', u'Circulatory_3', u'Digestive_3', u'Genitourinary_3', u'Poisoning_3', u'Muscoskeletal_3',\n",
    "           u'Neoplasms_3', u'Respiratory_3']\n",
    "    \n",
    "colsNonDiseases = [c for c in df_all.columns if c not in colsDiseases]\n",
    "\n",
    "print colsDiseases\n",
    "print colsNonDiseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] 42985 6293 22240\n"
     ]
    }
   ],
   "source": [
    "print df_all.loc[:,\"readmitted\"].sort_values().unique(), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 0), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 1), \\\n",
    "    np.sum(df_all[\"readmitted\"] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_readmisssion_vs_none\n",
      "[0 1] 42985 28533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Readmitted none vs readmitted\n",
    "print typeHypothesis\n",
    "\n",
    "if typeHypothesis == \"all_readmisssion_vs_none\":\n",
    "    df_all[\"readmitted\"][df_all[\"readmitted\"].values > 0] = 1\n",
    "    print df_all.iloc[:,-1].sort_values().unique(), \\\n",
    "            np.sum(df_all[\"readmitted\"] == 0), \\\n",
    "            np.sum(df_all[\"readmitted\"] == 1)\n",
    "            \n",
    "# Readmitted none vs early readmitted            \n",
    "if typeHypothesis == \"early_readmission_vs_none\":\n",
    "    df_all= df_all[df_all[\"readmitted\"].isin([0,1])]\n",
    "    print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute partition (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df_all, ts_thr=0.30):\n",
    "    y = df_all.readmitted\n",
    "    y = y.values\n",
    "\n",
    "    X = df_all.iloc[:,:-1].values\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=ts_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_partition(X_train, y_train, tr_thr=0.10):\n",
    "    X_train_aux = []\n",
    "    y_train_aux = []\n",
    "    sss = StratifiedShuffleSplit(y_train, 1, test_size=1-tr_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train_aux = X_train[train_index]\n",
    "        y_train_aux = y_train[train_index]\n",
    "\n",
    "    return X_train_aux, y_train_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute type fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_type_features(df_all):\n",
    "    if typeDataFeatures == \"minimum\":\n",
    "        numCols = ['time_in_hospital']\n",
    "\n",
    "    if typeDataFeatures == \"extended\":\n",
    "        numCols = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', \n",
    "                'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "    catCols = []\n",
    "    cols = df_all.columns\n",
    "    reducedCols = cols[:-1]\n",
    "\n",
    "    for i in range(len(cols)-1):\n",
    "        if cols[i] not in numCols:\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "    catCols = np.array(catCols)\n",
    "    \n",
    "    return catCols, reducedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(typeDisease):\n",
    "    if typeDisease == \"subset\":\n",
    "        return [\"subset\"]\n",
    "    else:\n",
    "        if typeDisease in colsDiseases:\n",
    "            return [typeDisease]\n",
    "        else:\n",
    "            return colsDiseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df_all, disease, typeDataExperiment):\n",
    "    if disease == \"subset\":\n",
    "        df_all_filtered = df_all.copy()\n",
    "    else:\n",
    "        cols_filtered = colsNonDiseases[:]\n",
    "        cols_filtered.insert(-1, disease)\n",
    "        df_all_filtered = df_all[cols_filtered].copy()    \n",
    "    \n",
    "    if typeDataExperiment == \"disease\" and disease != \"subset\":\n",
    "        df_all_filtered = df_all_filtered[df_all_filtered[disease] == 1]\n",
    "        df_all_filtered = df_all_filtered[[c for c in df_all_filtered.columns if c != disease]]\n",
    "    \n",
    "    return df_all_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms):\n",
    "    basePipeline = Pipeline([\n",
    "            (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "            (\"Scaler\", StandardScaler()),\n",
    "            (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "        ])\n",
    "\n",
    "    pipeline = [] \n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        for sm_type in sm_types:\n",
    "            for cls_method in cls_methods:\n",
    "                for lm in lms:\n",
    "                    if not (fs_method == \"rfe_rf_fs\" and cls_method == \"rf\"):\n",
    "                        params = {}   \n",
    "                        pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "                        if fs_method == \"combine_fs\":\n",
    "                            pipe.steps.insert(1,(fs_method, UnivCombineFilter(catCols,np.array(reducedCols))))\n",
    "                            params.update({fs_method + '__percentile':[5,10,20,30,40,50]})\n",
    "\n",
    "                        if fs_method == \"rfe_rf_fs\":\n",
    "                            pipe.steps.append((fs_method, RFE(estimator=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                                               n_estimators=100,\n",
    "                                                                                               random_state=33))))\n",
    "                            params.update({fs_method + '__step':[0.1]})\n",
    "                            params.update({fs_method + '__n_features_to_select':[\n",
    "                                                            int(len(reducedCols)*0.4), \n",
    "                                                            int(len(reducedCols)*0.6), \n",
    "                                                            int(len(reducedCols)*0.8)]})\n",
    "\n",
    "                        if fs_method == 'lasso_fs':\n",
    "                            pipe.steps.append((fs_method, SelectFromModel(\n",
    "                                        LogisticRegression(n_jobs=-1, penalty=\"l1\", dual=False, random_state=42))))\n",
    "                            params.update({fs_method + '__estimator__C': [0.001,0.01,0.1,1]})\n",
    "\n",
    "                        #Add classifiers\n",
    "                        if cls_method == \"knn\":\n",
    "                            pipe.steps.append((cls_method, KNeighborsClassifier()))\n",
    "                            params.update({'knn__n_neighbors':[1,3,5,7,9,11], 'knn__weights':['uniform', 'distance']})\n",
    "\n",
    "                        if cls_method == \"logReg\":\n",
    "                            pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "                            params.update({'logReg__C': [0.00001,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,15,30]})\n",
    "                            params.update({'logReg__class_weight': [None, 'balanced']})\n",
    "                            params.update({'logReg__penalty': ['l1', 'l2']})\n",
    "\n",
    "                        if cls_method == \"svmRBF\":\n",
    "                            pipe.steps.append((cls_method, SVC(random_state=42,probability=True)))\n",
    "                            params.update({'svmRBF__C': [0.01,0.1,0.5,1,5,10,30,50,100], \n",
    "                             'svmRBF__gamma' : [0.0001,0.001,0.01, 0.1,1,5]})\n",
    "                            params.update({'svmRBF__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,random_state=42)))\n",
    "                            params.update({'rf__n_estimators': [100,150,200,250,500], \n",
    "                                           'rf__criterion': ['entropy','gini'],\n",
    "                                           'rf__max_depth' : [None,4,6]})\n",
    "                            params.update({'rf__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"nb\":\n",
    "                             pipe.steps.append((cls_method, GaussianNB()))\n",
    "                             params.update({})\n",
    "                                               \n",
    "                        if cls_method == \"nn\":\n",
    "                            pipe.steps.append((cls_method, MLPClassifier(\n",
    "                                        activation='logistic',\n",
    "                                        solver='lbfgs', \n",
    "                                        hidden_layer_sizes=(5, 2), \n",
    "                                        random_state=13)))\n",
    "                            params.update({\n",
    "                                    'nn__alpha': [1e-5,0.00001,0.0001,0.001,0.01,0.1,1,3,5,10],\n",
    "                                    'nn__hidden_layer_sizes':[(30,),(50,),(70,),(100,),(150,),\n",
    "                                                              (30,30),(50,50),(70,70),(100,100),\n",
    "                                                              (30,30,30),(50,50,50),(70,70,70)\n",
    "                                                             ]\n",
    "                                          })\n",
    "\n",
    "                        #Add sampling\n",
    "                        pipe_imb = make_pipeline(*[p[1] for p in pipe.steps])\n",
    "                        stps = len(pipe_imb.steps)        \n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.remove(pipe_imb.steps[0])\n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.append(pipe.steps[s])\n",
    "\n",
    "                        if sm_type == \"after\":                    \n",
    "                            pipe_imb.steps.insert(stps - 1, \n",
    "                                                  (sm_method, SMOTE(ratio='auto', kind='regular', random_state=32)))\n",
    "                            params.update({sm_method + \"__k_neighbors\":[3,4,5]})\n",
    "\n",
    "\n",
    "                        pipeline.append([fs_method,sm_type,cls_method,lm,pipe_imb,params])\n",
    "    pipeline = pd.DataFrame(pipeline, columns=[\"fs\",\"sm\",\"cls\",\"metric\",\"pipe\",\"pipe_params\"])\n",
    "    pipeline.sort_values(\"fs\", inplace=True)\n",
    "    print pipeline.shape\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = [0.20,0.40,0.60] # 1.0\n",
    "ts_thr = 0.30\n",
    "\n",
    "fs_methods = [\"none\"] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"logReg\",\"nb\",\"rf\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\"]\n",
    "lms = [\"roc_auc\",\"recall\",\"f1_weighted\"] #[\"f1_weighted\",\"precision_weighted\"]\n",
    "sm_types = [\"none\"] #[\"none\",\"after\"]\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: (71518, 30)\n",
      "[u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1', u'Neoplasms_1', u'Respiratory_1']\n",
      "(9, 6)\n",
      "\n",
      "DataSet:\n",
      "**********\n",
      "**********\n",
      "SIZE: 0.2\n",
      "DISEASE: Diabetis_1\n",
      "(5805, 22)\n",
      "ALL TRAIN: (812, 21)\n",
      "TRAIN: [0's: 467 1's: 345 ]\n",
      "ALL TEST: (1742, 21)\n",
      "TEST: [0's: 1002 1's: 740 ]\n",
      "\n",
      "Num experiment: 0 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: logReg\n",
      "METRIC: roc_auc\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: roc_auc\n",
      "CV INNER selected params [None, 0.1, 'l2']\n",
      "CV INNER score: 0.633269230769\n",
      "\n",
      "CV OUTER f1 score: 0.620  (+/-0.013)\n",
      "CV OUTER prec score: 0.632  (+/-0.014)\n",
      "CV OUTER rec score: 0.637  (+/-0.012)\n",
      "Selected params (bests from CV) [None, 0.1, 'l2']\n",
      "\n",
      "TR F1 score: 0.655914283393\n",
      "TR Prec score: 0.671156294728\n",
      "TR Rec score: 0.67118226601\n",
      "\n",
      "Test f1: 0.606\n",
      "Test Precision: 0.619\n",
      "Test Recall: 0.611\n",
      "Test AUC: 0.592\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.77      0.70      1002\n",
      "          1       0.57      0.41      0.48       740\n",
      "\n",
      "avg / total       0.61      0.62      0.61      1742\n",
      "\n",
      "[[773 229]\n",
      " [434 306]]\n",
      "\n",
      "Total time: 4.84916186333\n",
      "\n",
      "Num experiment: 1 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: logReg\n",
      "METRIC: recall\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: recall\n",
      "CV INNER selected params ['balanced', 1e-05, 'l2']\n",
      "CV INNER score: 0.559615384615\n",
      "\n",
      "CV OUTER f1 score: 0.598  (+/-0.028)\n",
      "CV OUTER prec score: 0.602  (+/-0.028)\n",
      "CV OUTER rec score: 0.596  (+/-0.028)\n",
      "Selected params (bests from CV) ['balanced', 1e-05, 'l2']\n",
      "\n",
      "TR F1 score: 0.624516862448\n",
      "TR Prec score: 0.626961676271\n",
      "TR Rec score: 0.62315270936\n",
      "\n",
      "Test f1: 0.592\n",
      "Test Precision: 0.590\n",
      "Test Recall: 0.596\n",
      "Test AUC: 0.587\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.61      0.63      1002\n",
      "          1       0.52      0.56      0.54       740\n",
      "\n",
      "avg / total       0.60      0.59      0.59      1742\n",
      "\n",
      "[[610 392]\n",
      " [322 418]]\n",
      "\n",
      "Total time: 4.66026997566\n",
      "\n",
      "Num experiment: 2 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: logReg\n",
      "METRIC: f1_weighted\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: f1_weighted\n",
      "CV INNER selected params [None, 1, 'l2']\n",
      "CV INNER score: 0.602005363042\n",
      "\n",
      "CV OUTER f1 score: 0.615  (+/-0.015)\n",
      "CV OUTER prec score: 0.626  (+/-0.018)\n",
      "CV OUTER rec score: 0.631  (+/-0.016)\n",
      "Selected params (bests from CV) [None, 1, 'l2']\n",
      "\n",
      "TR F1 score: 0.658491704491\n",
      "TR Prec score: 0.674004589592\n",
      "TR Rec score: 0.673645320197\n",
      "\n",
      "Test f1: 0.608\n",
      "Test Precision: 0.621\n",
      "Test Recall: 0.613\n",
      "Test AUC: 0.594\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.78      0.70      1002\n",
      "          1       0.58      0.41      0.48       740\n",
      "\n",
      "avg / total       0.61      0.62      0.61      1742\n",
      "\n",
      "[[778 224]\n",
      " [436 304]]\n",
      "\n",
      "Total time: 4.88864207268\n",
      "\n",
      "Num experiment: 3 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: nb\n",
      "METRIC: roc_auc\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: roc_auc\n",
      "CV INNER selected params []\n",
      "CV INNER score: 0.63032967033\n",
      "\n",
      "CV OUTER f1 score: 0.612  (+/-0.013)\n",
      "CV OUTER prec score: 0.637  (+/-0.012)\n",
      "CV OUTER rec score: 0.638  (+/-0.009)\n",
      "Selected params (bests from CV) []\n",
      "\n",
      "TR F1 score: 0.621637523449\n",
      "TR Prec score: 0.647268317348\n",
      "TR Rec score: 0.646551724138\n",
      "\n",
      "Test f1: 0.574\n",
      "Test Precision: 0.606\n",
      "Test Recall: 0.596\n",
      "Test AUC: 0.565\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71      1002\n",
      "          1       0.57      0.30      0.39       740\n",
      "\n",
      "avg / total       0.60      0.61      0.57      1742\n",
      "\n",
      "[[834 168]\n",
      " [519 221]]\n",
      "\n",
      "Total time: 2.00461912155\n",
      "\n",
      "Num experiment: 4 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: nb\n",
      "METRIC: recall\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: recall\n",
      "CV INNER selected params []\n",
      "CV INNER score: 0.371153846154\n",
      "\n",
      "CV OUTER f1 score: 0.612  (+/-0.013)\n",
      "CV OUTER prec score: 0.637  (+/-0.012)\n",
      "CV OUTER rec score: 0.638  (+/-0.009)\n",
      "Selected params (bests from CV) []\n",
      "\n",
      "TR F1 score: 0.621637523449\n",
      "TR Prec score: 0.647268317348\n",
      "TR Rec score: 0.646551724138\n",
      "\n",
      "Test f1: 0.574\n",
      "Test Precision: 0.606\n",
      "Test Recall: 0.596\n",
      "Test AUC: 0.565\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71      1002\n",
      "          1       0.57      0.30      0.39       740\n",
      "\n",
      "avg / total       0.60      0.61      0.57      1742\n",
      "\n",
      "[[834 168]\n",
      " [519 221]]\n",
      "\n",
      "Total time: 2.04304504395\n",
      "\n",
      "Num experiment: 5 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: nb\n",
      "METRIC: f1_weighted\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER metric: f1_weighted\n",
      "CV INNER selected params []\n",
      "CV INNER score: 0.600873481626\n",
      "\n",
      "CV OUTER f1 score: 0.612  (+/-0.013)\n",
      "CV OUTER prec score: 0.637  (+/-0.012)\n",
      "CV OUTER rec score: 0.638  (+/-0.009)\n",
      "Selected params (bests from CV) []\n",
      "\n",
      "TR F1 score: 0.621637523449\n",
      "TR Prec score: 0.647268317348\n",
      "TR Rec score: 0.646551724138\n",
      "\n",
      "Test f1: 0.574\n",
      "Test Precision: 0.606\n",
      "Test Recall: 0.596\n",
      "Test AUC: 0.565\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71      1002\n",
      "          1       0.57      0.30      0.39       740\n",
      "\n",
      "avg / total       0.60      0.61      0.57      1742\n",
      "\n",
      "[[834 168]\n",
      " [519 221]]\n",
      "\n",
      "Total time: 2.06085896492\n",
      "\n",
      "Num experiment: 6 / 8\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: rf\n",
      "METRIC: roc_auc\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "subsetFeatures = get_features(typeDisease)\n",
    "print \"Total data:\", df_all.shape\n",
    "print subsetFeatures\n",
    "\n",
    "for tr_thr in tr_thrs:\n",
    "    for disease in subsetFeatures:\n",
    "        \n",
    "        df_all_filtered = get_dataset(df_all, disease, typeDataExperiment)               \n",
    "        X_train, X_test, y_train, y_test = train_test_partition(df_all_filtered, ts_thr)\n",
    "        X_train, y_train = train_partition(X_train, y_train, tr_thr)\n",
    "\n",
    "        catCols, reducedCols = compute_type_features(df_all_filtered)\n",
    "        pipeline = create_pipeline(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms)\n",
    "\n",
    "        print \"\\nDataSet:\"\n",
    "        print \"**********\"\n",
    "        print \"**********\"\n",
    "        print \"SIZE:\", tr_thr\n",
    "        print \"DISEASE:\", disease\n",
    "\n",
    "        print df_all_filtered.shape\n",
    "        print \"ALL TRAIN:\", X_train.shape\n",
    "        print \"TRAIN:\", \"[0's:\", np.sum(y_train==0), \"1's:\", np.sum(y_train==1), \"]\"\n",
    "        print \"ALL TEST:\", X_test.shape\n",
    "        print \"TEST:\", \"[0's:\", np.sum(y_test==0), \"1's:\", np.sum(y_test==1), \"]\"\n",
    "\n",
    "        for num_exp in range(pipeline.shape[0]):\n",
    "\n",
    "            # Run experiment\n",
    "            start = time.time()\n",
    "\n",
    "            #Prepare pipe_cls      \n",
    "            pipeline_cls = pipeline[\"pipe\"].iloc[num_exp]\n",
    "            pipeline_params = pipeline[\"pipe_params\"].iloc[num_exp]\n",
    "            fs = pipeline[\"fs\"].iloc[num_exp]\n",
    "            sm = pipeline[\"sm\"].iloc[num_exp]\n",
    "            cls = pipeline[\"cls\"].iloc[num_exp]\n",
    "            lm = pipeline[\"metric\"].iloc[num_exp]\n",
    "\n",
    "            print \"\\nNum experiment:\", str(num_exp), \"/\", str(pipeline.shape[0] - 1)\n",
    "            print \"****************\"\n",
    "\n",
    "            print \"FS:\",fs\n",
    "            print \"SM:\",sm\n",
    "            print \"CLS:\",cls\n",
    "            print \"METRIC:\",lm\n",
    "\n",
    "            #Prepare cv\n",
    "            cv_inner = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=24)\n",
    "            cv_outer = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=42)\n",
    "\n",
    "            #Fit pipeline with CV                        \n",
    "            grid_pipeline = GridSearchCV(pipeline_cls, param_grid=pipeline_params, verbose=verbose, \n",
    "                                         n_jobs=-1, cv=cv_inner, scoring= lm, error_score = 0) \n",
    "            grid_pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Compute pipeline evaluation with CVmetric\n",
    "            print \"\\nCV INNER metric: {}\".format(lm)\n",
    "            print \"CV INNER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "            print \"CV INNER score: {}\".format(grid_pipeline.best_score_)\n",
    "\n",
    "            cv_f1 = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_prec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='precision_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_rec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "            print \"\\nCV OUTER f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_f1), np.std(cv_f1))\n",
    "            print \"CV OUTER prec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_prec), np.std(cv_prec))\n",
    "            print \"CV OUTER rec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_rec), np.std(cv_rec))\n",
    "            print \"Selected params (bests from CV) {}\".format(grid_pipeline.best_params_.values())\n",
    "\n",
    "            # Computel Train score (with best CV params)\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_train)\n",
    "            train_prec_scores = metrics.precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "            train_rec_scores = metrics.recall_score(y_train, y_pred, average='weighted', pos_label=None)    \n",
    "            train_f1_scores = metrics.f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "            print \"\\nTR F1 score:\", train_f1_scores\n",
    "            print \"TR Prec score:\", train_prec_scores\n",
    "            print \"TR Rec score:\", train_rec_scores\n",
    "\n",
    "            #Compute test score\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_test)\n",
    "            test_f1 = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_prec = metrics.recall_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_rec = metrics.precision_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_auc = metrics.roc_auc_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            print \"\\nTest f1: %0.3f\" % (test_f1)\n",
    "            print \"Test Precision: %0.3f\" % (test_prec)\n",
    "            print \"Test Recall: %0.3f\" % (test_rec)\n",
    "            print \"Test AUC: %0.3f\" % (test_auc)\n",
    "\n",
    "            print \"with following performance in test:\"\n",
    "            print metrics.classification_report(y_test, y_pred)\n",
    "            print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            end = time.time()\n",
    "            print \"\\nTotal time:\", end - start\n",
    "            results = [num_exp,\n",
    "                           disease,\n",
    "                           typeEncounter,\n",
    "                           typeHypothesis,\n",
    "                           typeDataFeatures,\n",
    "                           typeDiagnosis,\n",
    "                           tr_thr,\n",
    "                           fs,\n",
    "                           sm,\n",
    "                           cls,\n",
    "                           lm,\n",
    "                           grid_pipeline.best_params_.values(),\n",
    "                           train_f1_scores,\n",
    "                           train_prec_scores,\n",
    "                           train_rec_scores,\n",
    "                           np.mean(cv_f1), \n",
    "                           np.std(cv_f1),\n",
    "                           np.mean(cv_prec), \n",
    "                           np.std(cv_prec),\n",
    "                           np.mean(cv_rec), \n",
    "                           np.std(cv_rec),                    \n",
    "                           test_f1,\n",
    "                           test_prec,\n",
    "                           test_rec,\n",
    "                           test_auc,                    \n",
    "                           end - start,\n",
    "                           grid_pipeline.best_estimator_\n",
    "                          ]\n",
    "\n",
    "            #Save results\n",
    "            df = pd.DataFrame(np.array(results).reshape(1,27), columns=\n",
    "                      [\"exp\", \"typeDisease\",\"typeEncounter\",\"typeHypothesis\",\"typeDataFeatures\",\"typeDiagnosis\",\n",
    "                       \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                       \"tr_f1\",\"tr_prec\",\"tr_rec\",\n",
    "                       \"cv_f1_mean\",\"cv_f1_std\",\"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                       \"test_f1\",\"test_prec\",\"test_rec\",\"test_auc\",\n",
    "                       \"time\",\"pipeline\"])\n",
    "\n",
    "            df.to_pickle(os.path.join(\"resources\", \"results\",\n",
    "                                      'results_pipe_' + \n",
    "                                      \"test_\" + str(ts_thr) + \"_\" +\n",
    "                                      \"train_\" + str(tr_thr) + \"_\" +\n",
    "                                      str(disease) + '_' +\n",
    "                                      str(typeEncounter) + '_' +\n",
    "                                      str(typeHypothesis) + '_' +\n",
    "                                      str(typeDataFeatures) + '_' +\n",
    "                                      str(typeDataExperiment) + '_' +\n",
    "                                      str(typeDiagnosis) + '_' +\n",
    "                                      str(fs) + '_' +\n",
    "                                      str(sm) + '_' +\n",
    "                                      str(lm) + '_' +\n",
    "                                      str(cls) + '_' +\n",
    "                                      '.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
