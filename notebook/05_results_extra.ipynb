{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter\n",
    "import MLpipeline as MLpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Local methods\n",
    "\n",
    "def load_data(typeEncounter, typeDiagnosis, typeDataFeatures):\n",
    "\n",
    "    if typeDataFeatures == \"non_extended\":\n",
    "        df_all=pd.read_pickle(os.path.join('resources','prepared_clean_data_' + typeEncounter + \"_\" +  typeDiagnosis + '.pkl'))\n",
    "    else:\n",
    "        df_all=pd.read_pickle(os.path.join('resources','prepared_clean_data_' + typeEncounter + \"_\" +  typeDiagnosis + '_' + typeDataFeatures + '.pkl'))\n",
    "\n",
    "\n",
    "    return df_all\n",
    "\n",
    "def get_columns(df_all, typeDiagnosis):\n",
    "\n",
    "    colsDiseases = []\n",
    "    if typeDiagnosis == \"diag_1\":\n",
    "        colsDiseases = [u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
    "               u'Neoplasms_1', u'Respiratory_1']\n",
    "\n",
    "    if typeDiagnosis == \"diag_3\":\n",
    "        colsDiseases = [u'Diabetis_3', u'Circulatory_3', u'Digestive_3', u'Genitourinary_3', u'Poisoning_3', u'Muscoskeletal_3',\n",
    "               u'Neoplasms_3', u'Respiratory_3']\n",
    "    \n",
    "    colsNonDiseases = [c for c in df_all.columns if c not in colsDiseases]\n",
    "    \n",
    "    return colsDiseases, colsNonDiseases\n",
    "\n",
    "def filter_data_by_class(df_all, typeHypothesis):\n",
    "    \n",
    "    # Readmitted none vs readmitted\n",
    "    if typeHypothesis == \"all_readmisssion_vs_none\":\n",
    "        df_all[\"readmitted\"][df_all[\"readmitted\"].values > 0] = 1\n",
    "\n",
    "    # Readmitted none vs early readmitted            \n",
    "    if typeHypothesis == \"early_readmission_vs_none\":\n",
    "        df_all= df_all[df_all[\"readmitted\"].isin([0,1])]\n",
    "        \n",
    "    return df_all\n",
    "\n",
    "def compute_type_features(df_all, typeDataFeatures):\n",
    "\n",
    "    numCols = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', \n",
    "                'number_emergency', 'number_inpatient', 'number_diagnoses',\n",
    "                'add_in_out', 'add_procs_meds', 'div_visits_time', 'div_em_time', 'div_visit_med', 'div_em_med',\n",
    "                'number_treatment','number_treatment_0','number_treatment_1','number_treatment_2','number_treatment_3']\n",
    "\n",
    "    catCols = []\n",
    "    cols = df_all.columns\n",
    "    reducedCols = cols[:-1]\n",
    "\n",
    "    for i in range(len(cols)-1):\n",
    "        if cols[i] not in numCols:\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "    catCols = np.array(catCols)\n",
    "    \n",
    "    return catCols, reducedCols\n",
    "\n",
    "def get_diseases(colsDiseases, typeDisease):\n",
    "    if typeDisease == \"subset\":\n",
    "        return [\"subset\"]\n",
    "    else:\n",
    "        if typeDisease in colsDiseases:\n",
    "            return [typeDisease]\n",
    "        else:\n",
    "            return colsDiseases\n",
    "\n",
    "def filter_data_by_diseases(df_all, disease, typeDataExperiment, colsNonDiseases):\n",
    "    if disease == \"subset\":\n",
    "        df_all_filtered = df_all.copy()\n",
    "    else:\n",
    "        cols_filtered = colsNonDiseases[:]\n",
    "        cols_filtered.insert(-1, disease)\n",
    "        df_all_filtered = df_all[cols_filtered].copy()    \n",
    "    \n",
    "    if typeDataExperiment == \"disease\" and disease != \"subset\":\n",
    "        df_all_filtered = df_all_filtered[df_all_filtered[disease] == 1]\n",
    "        df_all_filtered = df_all_filtered[[c for c in df_all_filtered.columns if c != disease]]\n",
    "    \n",
    "    return df_all_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generic methods\n",
    "\n",
    "def train_test_partition(df_all, ts_thr=0.30):\n",
    "    y = df_all.readmitted\n",
    "    y = y.values\n",
    "\n",
    "    X = df_all.iloc[:,:-1].values\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=ts_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_partition(X_train, y_train, tr_thr=0.10):\n",
    "    X_train_aux = []\n",
    "    y_train_aux = []\n",
    "    if tr_thr >= 1.0:\n",
    "            X_train_aux = X_train\n",
    "            y_train_aux = y_train\n",
    "    else:\n",
    "        sss = StratifiedShuffleSplit(y_train, 1, test_size=1-tr_thr, random_state=32) #random_state=42\n",
    "        for train_index, test_index in sss:\n",
    "            X_train_aux = X_train[train_index]\n",
    "            y_train_aux = y_train[train_index]\n",
    "\n",
    "    return X_train_aux, y_train_aux\n",
    "\n",
    "def create_pipelines(catCols,reducedCols, hyperparams, fs_methods, sm_method, sm_types, cls_methods, lms):\n",
    "    \n",
    "    basePipeline = Pipeline([\n",
    "            (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "            (\"Scaler\", StandardScaler()),\n",
    "            (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "        ])\n",
    "\n",
    "    pipeline = [] \n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        for sm_type in sm_types:\n",
    "            for cls_method in cls_methods:\n",
    "                for lm in lms:\n",
    "                    if not (fs_method == \"rfe_rf_fs\" and cls_method == \"rf\") and not(fs_method == \"lasso_fs\" and cls_method == \"logReg\"):\n",
    "                        params = {}   \n",
    "                        pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "                        if fs_method == \"combine_fs\":\n",
    "                            pipe.steps.insert(1,(fs_method, UnivCombineFilter(catCols,np.array(reducedCols))))\n",
    "                            pm = hyperparams[hyperparams[:,1] == fs_method,2]\n",
    "                            print fs_method,pm\n",
    "                            params.update(pm)                            \n",
    "\n",
    "\n",
    "                        if fs_method == \"rfe_rf_fs\":\n",
    "                            pipe.steps.append((fs_method, RFE(estimator=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                                               n_estimators=100,\n",
    "                                                                                               random_state=33))))\n",
    "\n",
    "                        if fs_method == 'lasso_fs':\n",
    "                            pipe.steps.append((fs_method, SelectFromModel(\n",
    "                                        LogisticRegression(n_jobs=-1, penalty=\"l1\", dual=False, random_state=42))))\n",
    "\n",
    "                        #Add classifiers\n",
    "                        if cls_method == \"knn\":\n",
    "                            pipe.steps.append((cls_method, KNeighborsClassifier()))\n",
    "\n",
    "                        if cls_method == \"logReg\":\n",
    "                            pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "\n",
    "                        if cls_method == \"svmRBF\":\n",
    "                            pipe.steps.append((cls_method, SVC(random_state=42,probability=True)))\n",
    "\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,class_weight='balanced',random_state=42)))\n",
    "\n",
    "                        if cls_method == \"gbt\":\n",
    "                            pipe.steps.append((cls_method, GradientBoostingClassifier(random_state=42,subsample=0.1,loss=\"deviance\")))\n",
    "\n",
    "\n",
    "                        if cls_method == \"nb\":\n",
    "                             pipe.steps.append((cls_method, GaussianNB()))\n",
    "                                               \n",
    "                        if cls_method == \"nn\":\n",
    "                            pipe.steps.append((cls_method, MLPClassifier(\n",
    "                                        activation='logistic',\n",
    "                                        solver='lbfgs', \n",
    "                                        hidden_layer_sizes=(5, 2), \n",
    "                                        random_state=13)))\n",
    "\n",
    "                            \n",
    "\n",
    "                        #Add sampling\n",
    "                        pipe_imb = make_pipeline(*[p[1] for p in pipe.steps])\n",
    "                        stps = len(pipe_imb.steps)        \n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.remove(pipe_imb.steps[0])\n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.append(pipe.steps[s])\n",
    "\n",
    "                        if sm_type == \"after\":                    \n",
    "                            pipe_imb.steps.insert(stps - 1, \n",
    "                                                  (sm_method, SMOTE(ratio='auto', kind='regular', random_state=32)))\n",
    "                            params.update({sm_method + \"__k_neighbors\":[3,4,5]})\n",
    "\n",
    "\n",
    "                        pipeline.append([fs_method,sm_type,cls_method,lm,pipe_imb,params])\n",
    "\n",
    "    pipelines = pd.DataFrame(pipeline, columns=[\"fs\",\"sm\",\"cls\",\"metric\",\"pipe\",\"pipe_params\"])\n",
    "    pipelines.sort_values(\"fs\", inplace=True)\n",
    "    print pipelines.shape\n",
    "    return pipelines\n",
    "\n",
    "\n",
    "# One Experiment One file \n",
    "def run(name,df_all, catCols, reducedCols, hyperparams, \n",
    "        ts_thr, tr_thrs, fs_methods, sm_method, sm_types, cls_methods, lms, cv_folds, cv_thr, \n",
    "        verbose=True, save=False):\n",
    "\n",
    "    results = []\n",
    "    for tr_thr in tr_thrs:\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_partition(df_all, ts_thr)\n",
    "            X_train, y_train = train_partition(X_train, y_train, tr_thr)\n",
    "            \n",
    "            pipeline = create_pipelines(catCols, reducedCols, fs_methods, sm_method, sm_types, cls_methods, lms)\n",
    "\n",
    "            print \"\\nDataSet:\"\n",
    "            print \"**********\"\n",
    "            print \"**********\"\n",
    "            print \"SIZE:\", tr_thr\n",
    "            print \"NAME:\", name\n",
    "\n",
    "            print df_all.shape\n",
    "            print \"ALL TRAIN:\", X_train.shape\n",
    "            print \"TRAIN:\", \"[0's:\", np.sum(y_train==0), \"1's:\", np.sum(y_train==1), \"]\"\n",
    "            print \"ALL TEST:\", X_test.shape\n",
    "            print \"TEST:\", \"[0's:\", np.sum(y_test==0), \"1's:\", np.sum(y_test==1), \"]\"\n",
    "\n",
    "            for num_exp in range(pipeline.shape[0]):\n",
    "\n",
    "                # Run experiment\n",
    "                start = time.time()\n",
    "\n",
    "                #Prepare pipe_cls      \n",
    "                pipeline_cls = pipeline[\"pipe\"].iloc[num_exp]\n",
    "                pipeline_params = pipeline[\"pipe_params\"].iloc[num_exp]\n",
    "                fs = pipeline[\"fs\"].iloc[num_exp]\n",
    "                sm = pipeline[\"sm\"].iloc[num_exp]\n",
    "                cls = pipeline[\"cls\"].iloc[num_exp]\n",
    "                lm = pipeline[\"metric\"].iloc[num_exp]\n",
    "\n",
    "                print \"\\nNum experiment:\", str(num_exp), \"/\", str(pipeline.shape[0] - 1)\n",
    "                print \"****************\"\n",
    "\n",
    "                print \"FS:\",fs\n",
    "                print \"SM:\",sm\n",
    "                print \"CLS:\",cls\n",
    "                print \"METRIC:\",lm\n",
    "\n",
    "                #Prepare cv\n",
    "                cv_inner = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=24)\n",
    "                cv_outer = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=42)\n",
    "\n",
    "                #Fit pipeline with CV                        \n",
    "                grid_pipeline = GridSearchCV(pipeline_cls, param_grid=pipeline_params, verbose=verbose, \n",
    "                                             n_jobs=-1, cv=cv_inner, scoring= lm, error_score = 0) \n",
    "                grid_pipeline.fit(X_train, y_train)\n",
    "\n",
    "                # Compute pipeline evaluation with CVmetric\n",
    "                print \"\\nCV INNER metric: {}\".format(lm)\n",
    "                print \"CV INNER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "                print \"CV INNER score: {}\".format(grid_pipeline.best_score_)\n",
    "\n",
    "                cv_f1 = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                         cv=cv_outer, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "                cv_prec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                         cv=cv_outer, scoring='precision_weighted', n_jobs=-1)\n",
    "\n",
    "                cv_rec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                         cv=cv_outer, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "                print \"\\nCV OUTER f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_f1), np.std(cv_f1))\n",
    "                print \"CV OUTER prec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_prec), np.std(cv_prec))\n",
    "                print \"CV OUTER rec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_rec), np.std(cv_rec))\n",
    "                print \"Selected params (bests from CV) {}\".format(grid_pipeline.best_params_.values())\n",
    "\n",
    "                # Computel Train score (with best CV params)\n",
    "                y_pred = grid_pipeline.best_estimator_.predict(X_train)\n",
    "                train_prec_scores = metrics.precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "                train_rec_scores = metrics.recall_score(y_train, y_pred, average='weighted', pos_label=None)    \n",
    "                train_f1_scores = metrics.f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "                print \"\\nTR F1 score:\", train_f1_scores\n",
    "                print \"TR Prec score:\", train_prec_scores\n",
    "                print \"TR Rec score:\", train_rec_scores\n",
    "\n",
    "                #Compute test score\n",
    "                y_pred = grid_pipeline.best_estimator_.predict(X_test)\n",
    "                test_f1_w = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "                test_prec_w = metrics.recall_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "                test_rec_w = metrics.precision_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "                test_auc_w = metrics.roc_auc_score(y_test, y_pred, average='weighted')\n",
    "                cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "                tn = cm[0,0]\n",
    "                fp = cm[0,1]\n",
    "                fn = cm[1,0]\n",
    "                tp = cm[1,1]\n",
    "                fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "                test_auc = metrics.auc(fpr, tpr)\n",
    "                test_spec = tn / float(tn + fp)\n",
    "                test_rec = tp / float(tp + fn)\n",
    "                test_prec = tp / float(tp + fp) \n",
    "\n",
    "                print \"\\nTest f1 (weighted): %0.3f\" % (test_f1_w)\n",
    "                print \"Test Precision (weighted): %0.3f\" % (test_prec_w)\n",
    "                print \"Test Recall / Sensitivity (weighted): %0.3f\" % (test_rec_w)\n",
    "                print \"Test AUC (weighted): %0.3f\" % (test_auc_w)\n",
    "                print\n",
    "                print \"Test Recall / Sensitivity: %0.3f\" % (test_rec)\n",
    "                print \"Test Specificity: %0.3f\" % (test_spec)\n",
    "                print \"Test Precision: %0.3f\" % (test_prec)\n",
    "                print \"Test AUC: %0.3f\" % (test_auc)\n",
    "                print \n",
    "                print \"with following performance in test:\"\n",
    "                print metrics.classification_report(y_test, y_pred)\n",
    "                print cm\n",
    "\n",
    "                end = time.time()\n",
    "                print \"\\nTotal time:\", end - start\n",
    "                res = [num_exp,\n",
    "                       name,\n",
    "                       tr_thr,\n",
    "                       fs,\n",
    "                       sm,\n",
    "                       cls,\n",
    "                       lm,\n",
    "                       grid_pipeline.best_params_.values(),\n",
    "                       train_f1_scores,\n",
    "                       train_prec_scores,\n",
    "                       train_rec_scores,\n",
    "                       np.mean(cv_f1), \n",
    "                       np.std(cv_f1),\n",
    "                       np.mean(cv_prec), \n",
    "                       np.std(cv_prec),\n",
    "                       np.mean(cv_rec), \n",
    "                       np.std(cv_rec),                    \n",
    "                       test_f1_w,\n",
    "                       test_prec_w,\n",
    "                       test_rec_w,\n",
    "                       test_auc_w,                    \n",
    "                       end - start,\n",
    "                       grid_pipeline.best_estimator_\n",
    "                      ]\n",
    "                results.append(res)\n",
    "\n",
    "                #Save results\n",
    "                if save:\n",
    "                    df = pd.DataFrame(np.array(res).reshape(1,27), columns=\n",
    "                          [\"exp\", \"typeDisease\",\"typeEncounter\",\"typeHypothesis\",\"typeDataFeatures\",\"typeDiagnosis\",\n",
    "                           \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                           \"tr_f1\",\"tr_prec\",\"tr_rec\",\n",
    "                           \"cv_f1_mean\",\"cv_f1_std\",\"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                           \"test_f1\",\"test_prec\",\"test_rec\",\"test_auc\",\n",
    "                           \"time\",\"pipeline\"])\n",
    "                \n",
    "                    df.to_pickle(os.path.join(\"resources\", \"results\",\n",
    "                                          'results_pipe_' + \n",
    "                                          \"test_\" + str(ts_thr) + \"_\" +\n",
    "                                          \"train_\" + str(tr_thr) + \"_\" +\n",
    "                                          str(name) + '_' +\n",
    "                                          str(fs) + '_' +\n",
    "                                          str(sm) + '_' +\n",
    "                                          str(lm) + '_' +\n",
    "                                          str(cls) + '_' +\n",
    "                                          time.strftime(\"%Y%m%d-%H%M%S\") +\n",
    "                                          '.pkl'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"last\" # ['first','last']\n",
    "typeHypothesis = \"early_readmission_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended_extra\" # [\"non_extended\",\"extended','extended_extra']\n",
    "    #Extended -> Subset of columns\n",
    "    #Minimum -> minimum set of columns \n",
    "typeDiagnosis = \"none\"  #[\"none\",\"diag_1\", \"diag_3\"]    \n",
    "typeDisease = \"subset\" # [\"subset\",\"any\",[\"Respiratory\",...]]\n",
    "    #subset -> Return subset of predefined disease features\n",
    "    #any -> Return all disease features    \n",
    "    #disease -> Return diseases feature\n",
    "typeDataExperiment = \"disease\" #[\"all\", \"disease\"] \n",
    "    #all -> Include all diagnosis as columns\n",
    "    #disease -> Remove diagnosis as column and keep only rows with diagnosis == 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = [0.01] # [0.1,0.2,0.4,0.6,1.0]\n",
    "ts_thr = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs_methods = [\"none\",] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"rf\",\"logReg\",\"knn\",\"nb\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\",\"gbt\"]\n",
    "lms = [\"recall\",\"f1\",\"f1_weighted\"] #[\"f1_weighted\",\"precision_weighted\",\"roc_auc\",\"recall\"]\n",
    "sm_types = [\"none\"] #[\"none\",\"after\"]\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 3)\n",
      "[['fs', 'combine_fs', {'combine_fs__percentile': [5, 10, 20, 30, 40, 50]}], ['fs', 'rfe_rf_fs', {'rfe_rf_fs__n_features_to_select': [5, 10, 15, 20], 'rfe_rf_fs__step': [0.1]}], ['fs', 'lasso_fs', {'lasso_fs__estimator__C': [0.001, 0.01, 0.1, 1]}], ['cls', 'knn', {'knn__weights': ['uniform', 'distance'], 'knn__n_neighbors': [1, 3, 5, 7, 9, 11]}], ['cls', 'logReg', {'logReg__class_weight': [None, 'balanced'], 'logReg__C': [1e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 15, 30], 'logReg__penalty': ['l1', 'l2']}], ['cls', 'svmRBF', {'svmRBF__gamma': [0.0001, 0.001, 0.01, 0.1, 1, 5], 'svmRBF__class_weight': [None, 'balanced'], 'svmRBF__C': [0.01, 0.1, 0.5, 1, 5, 10, 30, 50, 100]}], ['cls', 'rf', {'rf__criterion': ['entropy', 'gini'], 'rf__max_depth': [None, 4, 8, 12], 'rf__n_estimators': [200, 250, 300, 350, 400, 500]}], ['cls', 'nn', {'nn__hidden_layer_sizes': [(30,), (50,), (70,), (100,), (150,), (30, 30), (50, 50), (70, 70), (100, 100), (30, 30, 30), (50, 50, 50), (70, 70, 70)], 'nn__alpha': [1e-05, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 5, 10]}], ['cls', 'gbt', {'gbt__max_depth': [None, 8, 10, 12], 'gbt__learning_rate': [0.1, 0.01, 0.001], 'gbt__n_estimators': [300, 400, 500]}]]\n"
     ]
    }
   ],
   "source": [
    "hyperparams = np.load(\"../src/default_hyperparams.npy\")\n",
    "\n",
    "print hyperparams.shape\n",
    "print hyperparams.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAPE:\n",
      "(67182, 61)\n",
      "\n",
      "Initial columns:\n",
      "Index([u'gender', u'age', u'race_AfricanAmerican', u'race_Caucasian',\n",
      "       u'race_Other', u'HbA1c', u'Change', u'time_in_hospital', u'diabetesMed',\n",
      "       u'diss_home', u'medSpec_cardio', u'medSpec_Family/GeneralPractice',\n",
      "       u'medSpec_InternalMedicine', u'medSpec_surgery', u'adm_src_1',\n",
      "       u'adm_src_2', u'adm_src_3', u'adm_src_4', u'adm_src_5', u'adm_src_6',\n",
      "       u'adm_src_7', u'adm_src_8', u'adm_src_10', u'adm_src_11', u'adm_src_13',\n",
      "       u'adm_src_14', u'adm_src_22', u'adm_src_25', u'adm_1', u'adm_2',\n",
      "       u'adm_3', u'adm_4', u'adm_7', u'number_treatment',\n",
      "       u'num_lab_procedures', u'num_procedures', u'num_medications',\n",
      "       u'number_outpatient', u'number_emergency', u'number_inpatient',\n",
      "       u'number_diagnoses', u'insulin', u'metformin', u'pioglitazone',\n",
      "       u'glimepiride', u'glipizide', u'repaglinide', u'nateglinide',\n",
      "       u'ComplexHbA1c', u'add_in_out', u'add_procs_meds', u'div_visits_time',\n",
      "       u'div_em_time', u'div_visit_med', u'div_em_med', u'sum_ch_med',\n",
      "       u'number_treatment_0', u'number_treatment_1', u'number_treatment_2',\n",
      "       u'number_treatment_3', u'readmitted'],\n",
      "      dtype='object')\n",
      "\n",
      "Rows by class type:\n",
      "[0 1] 39785 5994\n",
      "\n",
      "Diseases: []\n",
      "\n",
      "Non-diseases: ['gender', 'age', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other', 'HbA1c', 'Change', 'time_in_hospital', 'diabetesMed', 'diss_home', 'medSpec_cardio', 'medSpec_Family/GeneralPractice', 'medSpec_InternalMedicine', 'medSpec_surgery', 'adm_src_1', 'adm_src_2', 'adm_src_3', 'adm_src_4', 'adm_src_5', 'adm_src_6', 'adm_src_7', 'adm_src_8', 'adm_src_10', 'adm_src_11', 'adm_src_13', 'adm_src_14', 'adm_src_22', 'adm_src_25', 'adm_1', 'adm_2', 'adm_3', 'adm_4', 'adm_7', 'number_treatment', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'insulin', 'metformin', 'pioglitazone', 'glimepiride', 'glipizide', 'repaglinide', 'nateglinide', 'ComplexHbA1c', 'add_in_out', 'add_procs_meds', 'div_visits_time', 'div_em_time', 'div_visit_med', 'div_em_med', 'sum_ch_med', 'number_treatment_0', 'number_treatment_1', 'number_treatment_2', 'number_treatment_3', 'readmitted']\n",
      "\n",
      "Total data: (45779, 61)\n",
      "['subset']\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df_all = MLpipeline.load_data(typeEncounter, typeDiagnosis, typeDataFeatures)\n",
    "print \"\\nSHAPE:\"\n",
    "print df_all.shape\n",
    "print \"\\nInitial columns:\"\n",
    "print df_all.columns\n",
    "\n",
    "#Filter data by class\n",
    "df_all = MLpipeline.filter_data_by_class(df_all, typeHypothesis)\n",
    "print \"\\nRows by class type:\"\n",
    "print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1)\n",
    "    \n",
    "#Get columns\n",
    "colsDiseases, colsNonDiseases = MLpipeline.get_columns(df_all,typeDiagnosis)\n",
    "print \"\\nDiseases:\", colsDiseases\n",
    "print \"\\nNon-diseases:\", colsNonDiseases\n",
    "    \n",
    "#Load diseases\n",
    "diseases = MLpipeline.get_diseases(colsDiseases, typeDisease)\n",
    "print \"\\nTotal data:\", df_all.shape\n",
    "print diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "(12, 6)\n"
     ]
    }
   ],
   "source": [
    "for disease in diseases:\n",
    "    \n",
    "    df_all_filtered = filter_data_by_diseases(df_all, disease, typeDataExperiment, colsNonDiseases)\n",
    "    catCols, reducedCols = compute_type_features(df_all_filtered, typeDataFeatures)\n",
    "    p = create_pipelines(catCols,reducedCols, hyperparams, fs_methods, sm_method, sm_types, cls_methods, lms)\n",
    "    p    \n",
    "    \"\"\"\n",
    "    name = disease + \"_\" + typeDataFeatures + \"_\" +  typeDataExperiment + \"_\" + typeEncounter + \"_\" +\n",
    "           typeHypothesis + \"_\" + typeDiagnosis\n",
    "        \n",
    "    res = MLpipeline.run(name, df_all_filtered, catCols, reducedCols, hyperparams, ts_thr, tr_thrs, \n",
    "                   fs_methods, sm_method, sm_types, \n",
    "                   cls_methods, lms, cv_folds, cv_thr, True, True)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
