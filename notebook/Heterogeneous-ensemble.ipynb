{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO RE-RUN\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required domain methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter\n",
    "import MLpipeline as MLpipeline\n",
    "import readmision_methods as rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"last\" # ['first','last']\n",
    "typeHypothesis = \"early_readmission_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended_extra_diag_3\" # [\"reduced\",\"extended','extended_extra','extended_extra_diag_1','extended_extra_diag_3']\n",
    "typeDataExperiment = \"all\" #[\"all\", \"disease\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = [1.0] # [0.1,0.2,0.4,0.6,1.0]\n",
    "ts_thr = 0.30\n",
    "\n",
    "fs_methods = [\"none\"] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"logReg\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\",\"gbt\"]\n",
    "lms = [\"recall\"] #[\"f1_weighted\",\"average_precision\",\"roc_auc\",\"recall\"]\n",
    "sm_types = [\"none\"] #[\"none\",\"after\"]\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAPE:\n",
      "(67182, 69)\n",
      "\n",
      "SHAPE FILTERED:\n",
      "(45779, 69)\n",
      "\n",
      "Rows by class type:\n",
      "[0 1] 39785 5994\n",
      "\n",
      "Train: (32045, 68) Test: (13734, 68)\n",
      "[['patient_filter', 5], ['admision_discharge_filter', 29], ['hospital_filter', 9], ['Visits_filter', 8], ['diagnosis_filter', 14], ['medicines_filter', 15], ['none_filter', 68]]\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df_all = rm.load_data(typeEncounter, typeDataFeatures)\n",
    "print \"\\nSHAPE:\"\n",
    "print df_all.shape\n",
    "\n",
    "#Filter data by class\n",
    "df_all = rm.filter_data_by_class(df_all, typeHypothesis)\n",
    "print \"\\nSHAPE FILTERED:\"\n",
    "print df_all.shape\n",
    "\n",
    "print \"\\nRows by class type:\"\n",
    "print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1)\n",
    "\n",
    "#Train & Test\n",
    "X_train, X_test, y_train, y_test = MLpipeline.train_test_partition(df_all)\n",
    "df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1,1))), columns=df_all.columns)\n",
    "df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1,1))), columns=df_all.columns)\n",
    "\n",
    "print \"\\nTrain:\", X_train.shape, \"Test:\",  X_test.shape\n",
    "\n",
    "#Create filters\n",
    "featFilters = rm.create_filters(df_all)\n",
    "print [[f[0],np.sum(f[1])] for f in featFilters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(df_train, ts_thr, cv_folds, cv_thr, fs_methods, sm_method, sm_types, \n",
    "                                            cls_methods, lms, featFilters):\n",
    "    #Split data\n",
    "    X_train_cv, X_test_ts, y_train_cv, y_test_ts = MLpipeline.train_test_partition(df_train, ts_thr)\n",
    "    df_train_cv = pd.DataFrame(np.hstack((X_train_cv, y_train_cv.reshape(-1,1))), columns=df_train.columns)\n",
    "    df_train_cv.readmitted =  df_train_cv.readmitted.astype(int)\n",
    "    \n",
    "    #Train inner models\n",
    "    models, cv_preds, ts_preds = train_inner_models(df_train, X_train_cv, X_test_ts, y_train_cv, y_test_ts, cv_folds, \n",
    "                                    cv_thr, fs_methods, sm_method, sm_types, cls_methods, lms, featFilters)\n",
    "    \n",
    "    #Train stack model\n",
    "    stacker = train_stacker(cv_preds, y_train_cv, ts_preds, y_test_ts)\n",
    "    \n",
    "    return models, stacker, cv_preds, y_train_cv, ts_preds, y_test_ts\n",
    "\n",
    "def train_inner_models(df_train_cv, X_train_cv, X_test_ts, y_train_cv, y_test_ts, cv_folds, cv_thr, fs_methods, \n",
    "                       sm_method, sm_types, cls_methods, lms, featFilters):\n",
    "    \n",
    "    models = []\n",
    "    cv_preds = []\n",
    "    ts_preds = []\n",
    "    for n,f in featFilters[:]:\n",
    "        \n",
    "        #Get categoric/numeric\n",
    "        f_cols = df_train_cv.columns[:-1][f==1].values.tolist()\n",
    "        f_cols.append(\"readmitted\")       \n",
    "        catCols, reducedCols = rm.compute_type_features(df_train_cv[f_cols])\n",
    "\n",
    "        #Get hyperparams\n",
    "        hyperparams = np.load(\"../src/default_hyperparams.npy\")\n",
    "\n",
    "        #Create pipeline\n",
    "        pipe = MLpipeline.create_pipelines(catCols, reducedCols, hyperparams, fs_methods, sm_method, sm_types, \n",
    "                                            cls_methods, lms, f)\n",
    "         \n",
    "        #Run pipeline\n",
    "        y_train_cv = y_train_cv.astype(int)\n",
    "        y_test_ts = y_test_ts.astype(int)\n",
    "        res = MLpipeline.run_pipeline(n, pipe, X_train_cv, X_test_ts, y_train_cv, y_test_ts, cv_folds, cv_thr, \n",
    "                                      verbose=True, save=False)[0]\n",
    "        \n",
    "        #Get prediction\n",
    "        tr_pred = res[-1].predict(X_train_cv).tolist()\n",
    "        ts_pred = res[-1].predict(X_test_ts).tolist()\n",
    "        \n",
    "        models.append(res)\n",
    "        cv_preds.append(tr_pred)\n",
    "        ts_preds.append(ts_pred)\n",
    "        \n",
    "    return models, cv_preds, ts_preds\n",
    "    \n",
    "    \n",
    "def train_stacker(cv_preds, y_cv, ts_preds, y_test):\n",
    "        \n",
    "    #Train stacker\n",
    "    cv_preds = np.array(cv_preds).astype(int).T\n",
    "    ts_preds = np.array(ts_preds).astype(int).T\n",
    "    y_cv = y_cv.astype(int)    \n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    #cls = SVC(random_state=13)\n",
    "    #params = {'kernel':['linear'], 'C':[1e-8,1e-5,0.001,0.01, 0.1, 1]}\n",
    "    \n",
    "    cls = RandomForestClassifier(random_state=13,)\n",
    "    params = {'n_estimators':[10,50,100,125,150,200,300],\"class_weight\":[\"balanced\"],\"max_depth\":[1,2]}\n",
    "    grid = GridSearchCV(cls, param_grid=params, verbose=True, \n",
    "                                 n_jobs=-1, cv=5, scoring= \"recall\", error_score = 0) \n",
    "    grid.fit(cv_preds, y_cv)    \n",
    "    y_pred = grid.best_estimator_.predict(ts_preds)\n",
    "    \n",
    "    print \"STACKER:\"\n",
    "    print \"*******\"\n",
    "    print \"y==0\", np.sum(y_test == 0)\n",
    "    print \"y==1\", np.sum(y_test == 1)    \n",
    "    print grid.best_params_\n",
    "    print grid.best_score_\n",
    "    \n",
    "    #Evaluate results\n",
    "    evaluate(\"train\", y_test, y_pred)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def evaluate(name, y_test, y_pred):\n",
    "\n",
    "    test_f1_w = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "    test_p, test_r, test_f1, test_s = metrics.precision_recall_fscore_support(y_test, y_pred,labels=None,average=None, sample_weight=None)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    test_auc = metrics.auc(fpr, tpr)                \n",
    "    cm_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn = cm_test[0,0]\n",
    "    fp = cm_test[0,1]\n",
    "    fn = cm_test[1,0]\n",
    "    tp = cm_test[1,1]\n",
    "    test_sens = test_r[1]\n",
    "    test_spec = tn / float(tn+fp)\n",
    "\n",
    "    print \"\\Evaluation:\", name\n",
    "    print \"*******************\"\n",
    "    \n",
    "    print \"TEST f1 (weighted): %0.3f\" % (test_f1_w)\n",
    "    print \"TEST Precision [c=0,1]:\", test_p\n",
    "    print \"TEST Recall [c=0,1]:\", test_r                \n",
    "    print \"TEST AUC: %0.3f\" % (test_auc)                \n",
    "    print \"TEST sensitivity:\", test_sens\n",
    "    print \"TEST Specificity:\", test_spec\n",
    "    print \"Confussion matrix:\"\n",
    "    print \"         | PRED\"\n",
    "    print \"REAL-->  v \"\n",
    "    print cm_test\n",
    "    \n",
    "def test_ensemble(df_test, models, stacker):\n",
    "    \n",
    "    test_preds = []\n",
    "    for m in models:\n",
    "        test_preds.append(m[-1].predict(df_test.iloc[:,:-1].values).tolist())\n",
    "    \n",
    "    test_preds = np.array(test_preds).astype(int).T\n",
    "    test_preds = test_preds.astype(int)\n",
    "    print test_preds.shape\n",
    "    y_pred = stacker.predict(test_preds)\n",
    "    y_test = df_test.iloc[:,-1].values.astype(int)\n",
    "    print type(y_pred), np.unique(y_pred)\n",
    "    print type(y_test), np.unique(y_test)\n",
    "    \n",
    "    #Evaluate results\n",
    "    evaluate(\"test\", y_test, y_pred.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataSet:\n",
      "**********\n",
      "**********\n",
      "SIZE: 5\n",
      "NAME: patient_filter\n",
      "ALL TRAIN: (22431, 68)\n",
      "TRAIN: [0's: 19494 1's: 2937 ]\n",
      "ALL TEST: (9614, 68)\n",
      "TEST: [0's: 8355 1's: 1259 ]\n",
      "\n",
      "Num experiment: 0 / 0\n",
      "****************\n",
      "FS: none\n",
      "SM: none\n",
      "CLS: logReg\n",
      "METRIC: recall\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    7.5s\n"
     ]
    }
   ],
   "source": [
    "#Train ensemble\n",
    "models, stacker, cv_preds, y_train_cv, ts_preds, y_test_ts = train_ensemble(df_train, 0.3, cv_folds, cv_thr, \n",
    "                                                                            fs_methods, sm_method, sm_types, \n",
    "                                                                            cls_methods, lms, featFilters)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test ensemble\n",
    "test_ensemble(df_test, models, stacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train stacker\n",
    "train_stacker(cv_preds, y_train_cv, ts_preds, y_test_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(np.array(models).reshape(-1,35), columns=\n",
    "                              [\"exp\", \"name\",\n",
    "                               \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                               \"tr_sens\",\"tr_spec\",\"tr_auc\",\n",
    "                               \"tr_prec\",\"tr_rec\",\"tr_f1\",\n",
    "                               \"cv_sens_mean\",\"cv_sens_std\",\"cv_spec_mean\",\"cv_spec_std\",\"cv_auc_mean\",\"cv_auc_std\",\n",
    "                               \"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                               \"cv_f1_mean\",\"cv_f1_std\",\n",
    "                               \"test_sens\",\"test_spec\",\"test_auc\",\n",
    "                               \"test_rec\",\"test_prec\",\"test_f1\",\n",
    "                               \"cm_test\",\n",
    "                               \"time\",\"pipeline\"])\n",
    "res[[\"name\",\"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\"tr_sens\",\"tr_spec\",\"tr_auc\",\n",
    "    \"tr_prec\",\"tr_rec\",\"tr_f1\",\"cv_sens_mean\",\"cv_sens_std\",\"cv_spec_mean\",\"cv_spec_std\",\"cv_auc_mean\",\"cv_auc_std\",\n",
    "    \"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\"cv_f1_mean\",\"cv_f1_std\",\n",
    "    \"test_sens\",\"test_spec\",\"test_auc\",\"test_rec\",\"test_prec\",\"test_f1\",\"cm_test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
