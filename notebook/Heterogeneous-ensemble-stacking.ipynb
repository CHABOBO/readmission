{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO RE-RUN\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilmira/.conda/envs/readmision/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.19.1.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid,cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.externals.joblib import Memory\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier, StackingClassifier\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required domain methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter\n",
    "import MLpipeline as MLpipeline\n",
    "from FeatFilter import FeatFilter\n",
    "import readmision_methods as rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"last\" # ['first','last']\n",
    "typeHypothesis = \"early_readmission_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended_extra_diag_3\" # [\"reduced\",\"extended','extended_extra','extended_extra_diag_1','extended_extra_diag_3']\n",
    "typeDataExperiment = \"all\" #[\"all\", \"disease\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = 0.7 # [0.1,0.2,0.4,0.6,1.0]\n",
    "ts_thr = 1 - tr_thrs\n",
    "\n",
    "selected_filters = range(6)\n",
    "fs_methods = [\"none\"] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"logReg\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\",\"gbt\"]\n",
    "lms = [\"recall\"] #[\"f1_weighted\",\"average_precision\",\"roc_auc\",\"recall\"]\n",
    "sm_types = [\"none\"] #[\"none\",\"after\"]tr_thrs\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAPE:\n",
      "(67182, 69)\n",
      "\n",
      "SHAPE FILTERED:\n",
      "(45779, 69)\n",
      "\n",
      "Rows by class type:\n",
      "[0 1] 39785 5994\n",
      "\n",
      "Train: (32045, 68) Test: (13734, 68)\n",
      "[['patient_filter', 5], ['admision_discharge_filter', 29], ['hospital_filter', 9], ['Visits_filter', 8], ['diagnosis_filter', 14], ['medicines_filter', 15], ['extra_filter', 68], ['none_filter', 17]]\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df_all = rm.load_data(typeEncounter, typeDataFeatures)\n",
    "print \"\\nSHAPE:\"\n",
    "print df_all.shape\n",
    "\n",
    "#Filter data by class\n",
    "df_all = rm.filter_data_by_class(df_all, typeHypothesis)\n",
    "print \"\\nSHAPE FILTERED:\"\n",
    "print df_all.shape\n",
    "\n",
    "print \"\\nRows by class type:\"\n",
    "print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1)\n",
    "\n",
    "#Train (on ts_thr percentage) & Test\n",
    "X_train, X_test, y_train, y_test = MLpipeline.train_test_partition(df_all,ts_thr)\n",
    "df_train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1,1))), columns=df_all.columns)\n",
    "df_test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1,1))), columns=df_all.columns)\n",
    "columns = df_all.columns\n",
    "\n",
    "print \"\\nTrain:\", X_train.shape, \"Test:\",  X_test.shape\n",
    "\n",
    "#Create filters\n",
    "featFilters = rm.create_filters(df_all)\n",
    "print [[f[0],np.sum(f[1])] for f in featFilters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pipelines(param_prefix, catCols,reducedCols, hyperparams, fs_methods, sm_method, sm_types, cls_methods, lms, featsToFilter=None):\n",
    "    \n",
    "    # Create a temporary folder to store the transformers of the pipeline\n",
    "    cachedir = mkdtemp()\n",
    "    memory = Memory(cachedir=cachedir, verbose=0)                        \n",
    "    \n",
    "    basePipeline = Pipeline([\n",
    "            (\"FeatFilter\", FeatFilter(featsToFilter)),\n",
    "            (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "            (\"Scaler\", StandardScaler()),\n",
    "            (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "        ])\n",
    "\n",
    "    pipeline = []\n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        for sm_type in sm_types:\n",
    "            for cls_method in cls_methods:\n",
    "                for lm in lms:\n",
    "\n",
    "                    params = {}\n",
    "                    pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "                    if cls_method == \"rf\":\n",
    "                        pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,random_state=42)))\n",
    "                        \n",
    "                    if cls_method == \"nb\":\n",
    "                        pipe.steps.append((cls_method, GaussianNB()))\n",
    "                        \n",
    "                    if cls_method == \"logReg\":\n",
    "                        pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "                       \n",
    "                    pm = hyperparams[hyperparams[:,1] == cls_method,2][0]\n",
    "                    new_pm = {}\n",
    "                    for key, values in pm.iteritems():\n",
    "                        key = param_prefix + key\n",
    "                        new_pm[key] = values\n",
    "                    params.update(new_pm) \n",
    "                        \n",
    "                    pipeline.append([fs_method,sm_type,cls_method,lm,pipe,params])       \n",
    "                    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(name, y_test, y_pred):\n",
    "\n",
    "    test_f1_w = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "    test_p, test_r, test_f1, test_s = metrics.precision_recall_fscore_support(y_test, y_pred,labels=None,average=None, sample_weight=None)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    test_auc = metrics.auc(fpr, tpr)                \n",
    "    cm_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn = cm_test[0,0]\n",
    "    fp = cm_test[0,1]\n",
    "    fn = cm_test[1,0]\n",
    "    tp = cm_test[1,1]\n",
    "    test_sens = test_r[1]\n",
    "    test_spec = tn / float(tn+fp)\n",
    "\n",
    "    print \"\\nEvaluation:\", name\n",
    "    print \"*******************\"\n",
    "    print          \n",
    "    print \"TEST AUC: %0.3f\" % (test_auc)                \n",
    "    print \"TEST sensitivity:\", test_sens\n",
    "    print \"TEST Specificity:\", test_spec\n",
    "    print\n",
    "    print \"TEST f1 (weighted): %0.3f\" % (test_f1_w)\n",
    "    print \"TEST f1 [c=0,1]\", test_f1\n",
    "    print \"TEST Precision [c=0,1]:\", test_p\n",
    "    print \"TEST Recall [c=0,1]:\", test_r  \n",
    "    \n",
    "    print \"Confussion matrix:\"\n",
    "    print \"         | PRED\"\n",
    "    print \"REAL-->  v \"\n",
    "    print cm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "[{'pipeline-1__logReg__penalty': ['l1', 'l2'], 'pipeline-1__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10], 'pipeline-1__logReg__class_weight': ['balanced']}, {'pipeline-2__logReg__class_weight': ['balanced'], 'pipeline-2__logReg__penalty': ['l1', 'l2'], 'pipeline-2__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10]}, {'pipeline-3__logReg__class_weight': ['balanced'], 'pipeline-3__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10], 'pipeline-3__logReg__penalty': ['l1', 'l2']}, {'pipeline-4__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10], 'pipeline-4__logReg__penalty': ['l1', 'l2'], 'pipeline-4__logReg__class_weight': ['balanced']}, {'pipeline-5__logReg__class_weight': ['balanced'], 'pipeline-5__logReg__penalty': ['l1', 'l2'], 'pipeline-5__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10]}, {'pipeline-6__logReg__class_weight': ['balanced'], 'pipeline-6__logReg__penalty': ['l1', 'l2'], 'pipeline-6__logReg__C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 3, 10]}, {'meta-randomforestclassifier__criterion': ['entropy', 'gini'], 'meta-randomforestclassifier__max_depth': [None, 2, 4, 6, 8], 'meta-randomforestclassifier__class_weight': ['balanced_subsample'], 'meta-randomforestclassifier__n_estimators': [50, 100, 200, 300, 500]}]\n"
     ]
    }
   ],
   "source": [
    "pipes = []\n",
    "params = []\n",
    "print selected_filters\n",
    "\n",
    "for i,sel in enumerate(selected_filters):\n",
    "    \n",
    "    n,f = featFilters[sel]\n",
    "    \n",
    "    #Get categoric/numeric\n",
    "    f_cols = columns[:-1][f==1].values.tolist()\n",
    "    f_cols.append(\"readmitted\")       \n",
    "    catCols, reducedCols = rm.compute_type_features(f_cols)\n",
    "\n",
    "    #Get hyperparams\n",
    "    hyperparams = np.load(\"../src/default_hyperparams.npy\")\n",
    "\n",
    "    #Create pipeline\n",
    "    pipe = create_pipelines(\"pipeline-\" + str(i+1) + \"__\", catCols, reducedCols, hyperparams, \n",
    "                            fs_methods, sm_method, sm_types, cls_methods, lms, f)\n",
    "\n",
    "    #Add results\n",
    "    pipes.append(pipe[0][-2])\n",
    "    params.append(pipe[0][-1])\n",
    "\n",
    "#Add meta hyperparams\n",
    "param_prefix = \"meta-\"\n",
    "meta_params = {}\n",
    "\n",
    "pm = hyperparams[hyperparams[:,1] == \"rf\",2][0]\n",
    "\n",
    "new_pm = {}\n",
    "for key, values in pm.iteritems():\n",
    "    key = param_prefix + key.replace(\"rf\",\"randomforestclassifier\")\n",
    "    new_pm[key] = values\n",
    "meta_params.update(new_pm)\n",
    "params.append(meta_params)\n",
    "\n",
    "\n",
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 146 candidates, totalling 730 fits\n"
     ]
    }
   ],
   "source": [
    "cv_inner = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=24)\n",
    "\n",
    "sclf = StackingClassifier(classifiers=pipes, verbose=0, use_probas=True,\n",
    "                            meta_classifier=RandomForestClassifier())    \n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=sclf, param_grid=params, n_jobs=-1,cv=cv_inner, \n",
    "                    scoring=\"recall\",refit=True, verbose=1,error_score = 0)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:  Train\n",
      "*******************\n",
      "\n",
      "f1-weighted score: 0.689  (+/-0.016)\n",
      "Prec score [c=0,1]: 0.918 (+/- 0.003), 0.203  (+/- 0.005)\n",
      "Rec  score [c=0,1]: 0.630 (+/- 0.026), 0.626  (+/- 0.027)\n",
      "AUC score: 0.675  (+/-0.003)\n",
      "Sensitivity score: 0.626  (+/-0.027)\n",
      "Specificity score: 0.630  (+/-0.026)\n",
      "\n",
      "Evaluation:  CV\n",
      "*******************\n",
      "\n",
      "f1-weighted score: 0.683  (+/-0.015)\n",
      "Prec score [c=0,1]: 0.910 (+/- 0.005), 0.193  (+/- 0.009)\n",
      "Rec  score [c=0,1]: 0.627 (+/- 0.022), 0.590  (+/- 0.022)\n",
      "AUC score: 0.649  (+/-0.015)\n",
      "Sensitivity score: 0.590  (+/-0.022)\n",
      "Specificity score: 0.627  (+/-0.022)\n",
      "\n",
      "Summary:\n",
      "*********\n",
      "StackingClassifier(average_probas=False,\n",
      "          classifiers=[Pipeline(memory=None,\n",
      "     steps=[('FeatFilter', FeatFilter(ixCols=array([1, 1, ..., 0, 0]))), ('Imputer', TypeFeatImputer(allNameCols=['gender', 'age', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other'],\n",
      "        dataCatCols=array([1, 1, 1, 1, 1]))), ('Scaler', StandardScaler(copy=...ty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])],\n",
      "          meta_classifier=RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
      "            criterion='entropy', max_depth=2, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=200, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False),\n",
      "          use_features_in_secondary=False, use_probas=True, verbose=0)\n",
      "{'meta-randomforestclassifier__criterion': 'entropy', 'meta-randomforestclassifier__max_depth': 2, 'meta-randomforestclassifier__class_weight': 'balanced_subsample', 'meta-randomforestclassifier__n_estimators': 200}\n",
      "0.572222222222\n",
      "[0.8, 's', 'none', '[0, 1, 2, 3, 4, 5]', 'recall', 'logReg', 0.68888066088045574, 0.015743927254236904, 0.91788736373519519, 0.0025590305252229336, 0.20314823795566053, 0.0051872164961058375, 0.62952056024420899, 0.026074585399372571, 0.62574493444576873, 0.026576855746686532, 0.67473946422720188, 0.0030060286060734622, 0.62574493444576873, 0.026576855746686532, 0.62952056024420899, 0.026074585399372571, 0.68340064578974735, 0.015064678586390245, 0.91025018519645473, 0.0046220113520888508, 0.19309674890477893, 0.0091212071968730449, 0.62739840804356928, 0.021938165664912464, 0.59000000000000008, 0.022470831573507412, 0.64930223897965822, 0.014813909382613495, 0.59000000000000008, 0.022470831573507412, 0.62739840804356928, 0.021938165664912464]\n"
     ]
    }
   ],
   "source": [
    "res = [ts_thr,sm_method[0],fs_methods[0],str(selected_filters),lms[0],cls_methods[0]]\n",
    "\n",
    "cv_outer = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=42)\n",
    "\n",
    "scorings = {'roc_auc': 'roc_auc',\n",
    "            'f1_weighted':'f1_weighted',\n",
    "            'precision_1':'precision',\n",
    "            'recall_1':'recall',\n",
    "            'precision_0' : metrics.make_scorer(MLpipeline.precision_0),\n",
    "            'recall_0' : metrics.make_scorer(MLpipeline.recall_0),\n",
    "            'spec': metrics.make_scorer(MLpipeline.specificity)\n",
    "           }\n",
    "cv_scores = cross_validate(grid.best_estimator_, X_train, y_train, \n",
    "                           cv=cv_outer, scoring=scorings, n_jobs=-1, return_train_score = True)\n",
    "\n",
    "for pre in [\"train\",\"test\"]:\n",
    "    cv_f1_w_mean = np.mean(cv_scores[pre + \"_f1_weighted\"])\n",
    "    cv_f1_w_std = np.std(cv_scores[pre + \"_f1_weighted\"])\n",
    "    cv_p1_mean = np.mean(cv_scores[pre + \"_precision_1\"])\n",
    "    cv_p1_std = np.std(cv_scores[pre + \"_precision_1\"])\n",
    "    cv_r1_mean = np.mean(cv_scores[pre + \"_recall_1\"])\n",
    "    cv_r1_std = np.std(cv_scores[pre + \"_recall_1\"])                \n",
    "    cv_p0_mean = np.mean(cv_scores[pre + \"_precision_0\"])\n",
    "    cv_p0_std = np.std(cv_scores[pre + \"_precision_0\"])\n",
    "    cv_r0_mean = np.mean(cv_scores[pre + \"_recall_0\"])\n",
    "    cv_r0_std = np.std(cv_scores[pre + \"_recall_0\"])\n",
    "\n",
    "    cv_auc_mean = np.mean(cv_scores[pre + \"_roc_auc\"])\n",
    "    cv_auc_std = np.std(cv_scores[pre + \"_roc_auc\"])                \n",
    "    cv_spec_mean = np.mean(cv_scores[pre + \"_spec\"])\n",
    "    cv_spec_std = np.std(cv_scores[pre + \"_spec\"])\n",
    "    cv_sens_mean = cv_r1_mean\n",
    "    cv_sens_std = cv_r1_std\n",
    "\n",
    "    print \"\\nEvaluation: \", \"CV\" if pre == \"test\" else \"Train\"\n",
    "    print \"*******************\"\n",
    "    print \"\\nf1-weighted score: %0.3f  (+/-%0.03f)\" % (cv_f1_w_mean,cv_f1_w_std)               \n",
    "    print \"Prec score [c=0,1]: {:.3f} (+/- {:.3f}), {:.3f}  (+/- {:.3f})\".format(cv_p0_mean,cv_p0_std,cv_p1_mean,cv_p1_std)                \n",
    "    print \"Rec  score [c=0,1]: {:.3f} (+/- {:.3f}), {:.3f}  (+/- {:.3f})\".format(cv_r0_mean,cv_r0_std,cv_r1_mean,cv_r1_std)\n",
    "    print \"AUC score: %0.3f  (+/-%0.03f)\" % (cv_auc_mean,cv_auc_std) \n",
    "    print \"Sensitivity score: %0.3f  (+/-%0.03f)\" % (cv_sens_mean,cv_sens_std) \n",
    "    print \"Specificity score: %0.3f  (+/-%0.03f)\" % (cv_spec_mean,cv_spec_std)\n",
    "    \n",
    "    res.extend([cv_f1_w_mean,cv_f1_w_std,\n",
    "                cv_p0_mean,cv_p0_std,cv_p1_mean,cv_p1_std,\n",
    "                cv_r0_mean,cv_r0_std,cv_r1_mean,cv_r1_std,\n",
    "                cv_auc_mean,cv_auc_std,\n",
    "                cv_sens_mean,cv_sens_std,\n",
    "                cv_spec_mean,cv_spec_std\n",
    "               ])\n",
    "\n",
    "\n",
    "print \"\\nSummary:\"\n",
    "print \"*********\"\n",
    "print grid.best_estimator_\n",
    "print grid.best_params_\n",
    "print grid.best_score_\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: test\n",
      "*******************\n",
      "\n",
      "TEST AUC: 0.610\n",
      "TEST sensitivity: 0.578727841502\n",
      "TEST Specificity: 0.640485092212\n",
      "\n",
      "TEST f1 (weighted): 0.692\n",
      "TEST f1 [c=0,1] [ 0.75176547  0.29190554]\n",
      "TEST Precision [c=0,1]: [ 0.90984558  0.19517513]\n",
      "TEST Recall [c=0,1]: [ 0.64048509  0.57872784]\n",
      "Confussion matrix:\n",
      "         | PRED\n",
      "REAL-->  v \n",
      "[[20386 11443]\n",
      " [ 2020  2775]]\n"
     ]
    }
   ],
   "source": [
    "#Predict test data    \n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "#Evaluate results\n",
    "evaluate(\"test\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_thr</th>\n",
       "      <th>sm</th>\n",
       "      <th>fs</th>\n",
       "      <th>pipes</th>\n",
       "      <th>metric</th>\n",
       "      <th>cls</th>\n",
       "      <th>tr_f1_w_mean</th>\n",
       "      <th>tr_f1_w_std</th>\n",
       "      <th>tr_p0_mean</th>\n",
       "      <th>tr_p0_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cv_r0_mean</th>\n",
       "      <th>cv_r0_std</th>\n",
       "      <th>cv_r1_mean</th>\n",
       "      <th>cv_r1_std</th>\n",
       "      <th>cv_auc_mean</th>\n",
       "      <th>cv_auc_std</th>\n",
       "      <th>cv_sens_mean</th>\n",
       "      <th>cv_sens_std</th>\n",
       "      <th>cv_spec_mean</th>\n",
       "      <th>cv_spec_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>s</td>\n",
       "      <td>none</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>recall</td>\n",
       "      <td>logReg</td>\n",
       "      <td>0.68888066088</td>\n",
       "      <td>0.0157439272542</td>\n",
       "      <td>0.917887363735</td>\n",
       "      <td>0.00255903052522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627398408044</td>\n",
       "      <td>0.0219381656649</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0224708315735</td>\n",
       "      <td>0.64930223898</td>\n",
       "      <td>0.0148139093826</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0224708315735</td>\n",
       "      <td>0.627398408044</td>\n",
       "      <td>0.0219381656649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ts_thr sm    fs               pipes  metric     cls   tr_f1_w_mean  \\\n",
       "0    0.8  s  none  [0, 1, 2, 3, 4, 5]  recall  logReg  0.68888066088   \n",
       "\n",
       "       tr_f1_w_std      tr_p0_mean         tr_p0_std       ...         \\\n",
       "0  0.0157439272542  0.917887363735  0.00255903052522       ...          \n",
       "\n",
       "       cv_r0_mean        cv_r0_std cv_r1_mean        cv_r1_std    cv_auc_mean  \\\n",
       "0  0.627398408044  0.0219381656649       0.59  0.0224708315735  0.64930223898   \n",
       "\n",
       "        cv_auc_std cv_sens_mean      cv_sens_std    cv_spec_mean  \\\n",
       "0  0.0148139093826         0.59  0.0224708315735  0.627398408044   \n",
       "\n",
       "       cv_spec_std  \n",
       "0  0.0219381656649  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "df_aux = pd.DataFrame(np.array(res).reshape(1,38),columns=[\"ts_thr\",\"sm\",\"fs\",\"pipes\",\"metric\",\"cls\",\n",
    "\"tr_f1_w_mean\",\"tr_f1_w_std\",\"tr_p0_mean\",\"tr_p0_std\",\"tr_p1_mean\",\"tr_p1_std\",\n",
    "\"tr_r0_mean\",\"tr_r0_std\",\"tr_r1_mean\",\"tr_r1_std\",\"tr_auc_mean\",\"tr_auc_std\",\n",
    "\"tr_sens_mean\",\"tr_sens_std\",\"tr_spec_mean\",\"tr_spec_std\",\n",
    "\"cv_f1_w_mean\",\"cv_f1_w_std\",\"cv_p0_mean\",\"cv_p0_std\",\"cv_p1_mean\",\"cv_p1_std\",\n",
    "\"cv_r0_mean\",\"cv_r0_std\",\"cv_r1_mean\",\"cv_r1_std\",\"cv_auc_mean\",\"cv_auc_std\",\n",
    "\"cv_sens_mean\",\"cv_sens_std\",\"cv_spec_mean\",\"cv_spec_std\"                                                       \n",
    " ])\n",
    "\n",
    "df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat to all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_thr</th>\n",
       "      <th>sm</th>\n",
       "      <th>fs</th>\n",
       "      <th>pipes</th>\n",
       "      <th>metric</th>\n",
       "      <th>cls</th>\n",
       "      <th>tr_f1_w_mean</th>\n",
       "      <th>tr_f1_w_std</th>\n",
       "      <th>tr_p0_mean</th>\n",
       "      <th>tr_p0_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cv_r0_mean</th>\n",
       "      <th>cv_r0_std</th>\n",
       "      <th>cv_r1_mean</th>\n",
       "      <th>cv_r1_std</th>\n",
       "      <th>cv_auc_mean</th>\n",
       "      <th>cv_auc_std</th>\n",
       "      <th>cv_sens_mean</th>\n",
       "      <th>cv_sens_std</th>\n",
       "      <th>cv_spec_mean</th>\n",
       "      <th>cv_spec_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>s</td>\n",
       "      <td>none</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>recall</td>\n",
       "      <td>logReg</td>\n",
       "      <td>0.635542545584</td>\n",
       "      <td>0.00958233907371</td>\n",
       "      <td>0.913019442175</td>\n",
       "      <td>0.00350401660316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546962714705</td>\n",
       "      <td>0.0134022826994</td>\n",
       "      <td>0.616111111111</td>\n",
       "      <td>0.0386260962128</td>\n",
       "      <td>0.610293371503</td>\n",
       "      <td>0.0161283961155</td>\n",
       "      <td>0.616111111111</td>\n",
       "      <td>0.0386260962128</td>\n",
       "      <td>0.546962714705</td>\n",
       "      <td>0.0134022826994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>s</td>\n",
       "      <td>none</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>recall</td>\n",
       "      <td>logReg</td>\n",
       "      <td>0.68888066088</td>\n",
       "      <td>0.0157439272542</td>\n",
       "      <td>0.917887363735</td>\n",
       "      <td>0.00255903052522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627398408044</td>\n",
       "      <td>0.0219381656649</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0224708315735</td>\n",
       "      <td>0.64930223898</td>\n",
       "      <td>0.0148139093826</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0224708315735</td>\n",
       "      <td>0.627398408044</td>\n",
       "      <td>0.0219381656649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ts_thr sm    fs               pipes  metric     cls    tr_f1_w_mean  \\\n",
       "0    0.8  s  none           [0, 1, 2]  recall  logReg  0.635542545584   \n",
       "0    0.8  s  none  [0, 1, 2, 3, 4, 5]  recall  logReg   0.68888066088   \n",
       "\n",
       "        tr_f1_w_std      tr_p0_mean         tr_p0_std       ...         \\\n",
       "0  0.00958233907371  0.913019442175  0.00350401660316       ...          \n",
       "0   0.0157439272542  0.917887363735  0.00255903052522       ...          \n",
       "\n",
       "       cv_r0_mean        cv_r0_std      cv_r1_mean        cv_r1_std  \\\n",
       "0  0.546962714705  0.0134022826994  0.616111111111  0.0386260962128   \n",
       "0  0.627398408044  0.0219381656649            0.59  0.0224708315735   \n",
       "\n",
       "      cv_auc_mean       cv_auc_std    cv_sens_mean      cv_sens_std  \\\n",
       "0  0.610293371503  0.0161283961155  0.616111111111  0.0386260962128   \n",
       "0   0.64930223898  0.0148139093826            0.59  0.0224708315735   \n",
       "\n",
       "     cv_spec_mean      cv_spec_std  \n",
       "0  0.546962714705  0.0134022826994  \n",
       "0  0.627398408044  0.0219381656649  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,df_aux])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf, os.path.join('resources','results','ensemble_train_0_2_inner_logReg_outer_rf_3_pipes.pkl')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"res_ensemble_20171031-124424\"\n",
    "#filename = 'res_ensemble_' + time.strftime(\"%Y%m%d-%H%M%S\") + '.pkl'\n",
    "\n",
    "df.to_pickle(os.path.join('resources','results', filename + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join('resources','results',\"res_ensemble_20171031-124424.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
