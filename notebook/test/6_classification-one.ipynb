{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "from sklearn import svm, datasets, feature_selection, cross_validation\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, SelectFdr, SelectFpr\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, RFECV, RFE, mutual_info_classif\n",
    "from sklearn.feature_selection import VarianceThreshold, f_regression\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.learning_curve import validation_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import feature_selection\n",
    "from sklearn import cross_validation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,Imputer,Normalizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA, RandomizedPCA, KernelPCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from pylab import pcolor, show, colorbar, xticks, yticks\n",
    "from numpy import corrcoef, sum, log, arange\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import logging.config\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"/home/aegle/projects/myosa/src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from i_score_parallel import i_score\n",
    "from TypeFeatFS import DiscreteFS, ContinuousFS\n",
    "from TypeFeatFilter import DiscreteFilter, ContinuousFilter\n",
    "from OutlierFiltering import OutlierFiltering\n",
    "from typeFeat_score import typeFeat_score\n",
    "from UnivCombineFilter import UnivCombineFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = 2\n",
    "suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 70) (42, 1) (42,)\n",
      "(29, 70) (13, 70)\n",
      "['GENERO' 'ACTIVO' 'JUBILADO' 'EDAD' 'FUMA' 'FUMA_PAQ_ANY' 'FUMA_EX_ANYS'\n",
      " 'ALCOHOL' 'ALCOHOL_GR_DIA' 'CAFES_DIA' 'DEPRESION' 'ANSIEDAD' 'HTA'\n",
      " 'CARDIOPATIA' 'ENF_RESP' 'DIABETES' 'OBESIDAD' 'DISLIPEMIA'\n",
      " 'OBSTR_NASAL_CRON' 'RONCA' 'SUE_REP' 'CRISIS_ASF' 'NICTURIA' 'APNEAS_PRES'\n",
      " 'CEFALEA' 'SOMN_DIURN' 'DESPERTAR_NOCT' 'TRAST_CONC' 'IRR_APAT_DEPR'\n",
      " 'INSOMNIO' 'ACT_MOTR_NOCT' 'SEN_SUE_REAL_DORM' 'SUE_INT_PIROSIS'\n",
      " 'DISM_DESEO_SEX' 'IECAS' 'DIURETICOS' 'ANTIAGREG' 'ANTIACID' 'HIPOLIPEM'\n",
      " 'BDZ' 'TIEMP_SUENO' 'IAH' 'TIEMPOSAT<90%' 'IND_DESAT' 'TALLA' 'IMC'\n",
      " 'CIRC_CUELLO' 'MEDIA_TAS' 'MEDIA_TAD' 'Sat O2' 'PRESS_CPAP' 'EPWORTH'\n",
      " 'EUROQOL' 'VISUAL' 'HORAS_USO_TOT_1' 'HORAS_USO_MED_NOCHE_1'\n",
      " 'AL_IRR_CUT_MASC_1' 'BOCA_SECA_1' 'MEDIA_TAS_1' 'MEDIA_TAD_1' 'EPWORTH_1'\n",
      " 'EUROQOL_1' 'VISUAL_EUROQOL_1' 'HORAS_USO_MED_NOCHE_3' 'BOCA_SECA_3'\n",
      " 'MEDIA TAS_3' 'MEDIA TAD_3' 'EPWORTH_3' 'EUROQOL_3' 'VISUAL EUROQOL_3'\n",
      " 'LABEL']\n"
     ]
    }
   ],
   "source": [
    "datafilenames = []\n",
    "datafilenames.append(os.path.join('resources','data_partition_cl0_short_no_monit_14012016{}.pkl'.format(suffix)))\n",
    "datafilenames.append(os.path.join('resources','data_partition_cl0_short_1m_monit_14012016{}.pkl'.format(suffix)))\n",
    "datafilenames.append(os.path.join('resources','data_partition_cl0_short_3m_1m_monit_14012016{}.pkl'.format(suffix)))\n",
    "\n",
    "\n",
    "#Load Train & test data\n",
    "f = file(datafilenames[ds],\"rb\")\n",
    "X_train = np.load(f).astype(float)\n",
    "y_train = np.load(f).astype(int)\n",
    "X_test = np.load(f).astype(float)\n",
    "y_test = np.load(f).astype(int)\n",
    "cols = np.load(f)\n",
    "tr_codes = np.load(f)\n",
    "ts_codes = np.load(f)\n",
    "feat_types = np.load(f)\n",
    "feat_types = dict(feat_types.tolist())\n",
    "f.close()\n",
    "\n",
    "#Join data\n",
    "X_all = np.vstack((X_train,X_test))\n",
    "y_all = np.hstack((y_train, y_test)).reshape(-1,1)\n",
    "all_codes = np.hstack((tr_codes,ts_codes))\n",
    "print X_all.shape, y_all.shape, all_codes.shape\n",
    "print X_train.shape, X_test.shape\n",
    "print cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Find categorical (discrete) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat cols: 38\n",
      "Num cols: 32\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "catCols = []\n",
    "reducedCols = cols[:-1]\n",
    "\n",
    "for i in range(len(cols)-1):\n",
    "    if feat_types[cols[i]] == 'object' \\\n",
    "        or cols[i] in (\"RONCA\",\"SUE_REP\",\"CRISIS_ASF\",\"NICTURIA\",\"APNEAS_PRES\",\"CEFALEA\",\n",
    "                \"SOMN_DIURN\",\"DESPERTAR_NOCT\",\"TRAST_MEM\",\"TRAST_CONC\",\"IRR_APAT_DEPR\"):\n",
    "        catCols.append(1)\n",
    "    else:\n",
    "        catCols.append(0)\n",
    "catCols = np.array(catCols)\n",
    "\n",
    "print \"Cat cols:\", np.sum(catCols==1)\n",
    "print \"Num cols:\", np.sum(catCols==0)\n",
    "print len(reducedCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Clean irrelevant features (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "to_del_name = []\n",
    "to_del_ix = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Null values > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN by individual > 30%: 0\n",
      "Number of NaN by column > 30%: 0\n",
      "No features to del with high NAN values\n"
     ]
    }
   ],
   "source": [
    "#Find columns with NaN   \n",
    "naUsers = np.hstack((all_codes.reshape((-1,1)), \\\n",
    "          (np.sum(np.isnan(X_all.tolist()), axis=1) / float(X_all.shape[1])).reshape(len(X_all),1)))\n",
    "\n",
    "df_naUsers = pd.DataFrame(data = naUsers, columns = ['users', 'NumNaNCols'])\n",
    "print \"Number of NaN by individual > 30%:\", np.sum(df_naUsers[\"NumNaNCols\"] > 0.3)\n",
    "\n",
    "#Filter columns with NAN > 30%    \n",
    "nanColumns = np.sum(np.isnan(X_all.tolist()), axis=0) / float(X_all.shape[0])\n",
    "naVals = np.hstack((cols[:-1].reshape(-1,1), nanColumns.reshape(-1,1)))\n",
    "df_naVals = pd.DataFrame(data = naVals, columns = ['columnNames', 'NaN_perc'])\n",
    "print \"Number of NaN by column > 30%:\", np.sum(df_naVals[\"NaN_perc\"] > 0.3)\n",
    "\n",
    "to_del = []\n",
    "for h, col in enumerate(nanColumns):\n",
    "    if col > 0.3:\n",
    "        print \"DELETE column with high NAN values (30%):\", cols[h]\n",
    "        to_del.append(cols[h])\n",
    "\n",
    "if len(to_del) == 0:\n",
    "    print \"No features to del with high NAN values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Imput nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "imputer = TypeFeatImputer(catCols, reducedCols)\n",
    "imputer.fit(X_all, y_all)\n",
    "X_Imp = imputer.transform(X_all)\n",
    "X_filt = X_Imp\n",
    "\n",
    "print \"Nulls after imputation:\", np.sum(np.sum(np.isnan(X_Imp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Low variant features Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 70)\n",
      "Filtered columns: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "varFilter = VarianceThreshold(threshold=0.0)\n",
    "varFilter.fit(X_Imp)\n",
    "\n",
    "to_del_name.extend(cols[:-1][varFilter.get_support() == 0])\n",
    "to_del_ix.extend(np.where(varFilter.get_support() == 0)[0])\n",
    "\n",
    "print X_filt.shape\n",
    "print \"Filtered columns:\", np.sum(varFilter.get_support()==False)\n",
    "print to_del_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar features ACTIVO and JUBILADO score: -0.857492925713\n",
      "Class significance of ACTIVO is 0.414979503738\n",
      "Class significance of JUBILADO is 0.0148575446864\n",
      "\n",
      "Similar features ALCOHOL and ALCOHOL_GR_DIA score: 0.874313210259\n",
      "Class significance of ALCOHOL is 0.0974316203361\n",
      "Class significance of ALCOHOL_GR_DIA is 0.27947537567\n",
      "Columns to delete: 2 ['ACTIVO', 'ALCOHOL_GR_DIA']\n",
      "[1, 8]\n"
     ]
    }
   ],
   "source": [
    "cols_corr = []\n",
    "p_scores = []\n",
    "p_scores_all = []\n",
    "v_th = 0.15\n",
    "\n",
    "for i in range(X_Imp.shape[1]):\n",
    "    for j in range(X_Imp.shape[1]):\n",
    "        if i != j and (i,j) not in cols_corr and (j,i) not in cols_corr:\n",
    "            \n",
    "            cols_corr.append((i,j))\n",
    "            coeff_corr, p_score = spearmanr(X_Imp[:,i], X_Imp[:,j])\n",
    "            \n",
    "            if float(coeff_corr) > (1.0 - v_th) or float(coeff_corr) < (v_th - 1.0) :\n",
    "                sc1, pval1 = mannwhitneyu(X_Imp[:,i].reshape(-1,1),y_all.reshape(-1,1))\n",
    "                sc2, pval2 = mannwhitneyu(X_Imp[:,j].reshape(-1,1),y_all.reshape(-1,1))\n",
    "                \n",
    "                print \"\\nSimilar features\",cols[i],\"and\",cols[j],\"score:\", coeff_corr\n",
    "                print \"Class significance of\", cols[i], \"is\", pval1\n",
    "                print \"Class significance of\", cols[j], \"is\", pval2\n",
    "                \n",
    "                to_del_ix.append(j if pval1 < pval2 else i)\n",
    "                to_del_name.append(cols[j if pval1 < pval2 else i])\n",
    "                p_scores.append([i, j, coeff_corr, p_score])\n",
    "                \n",
    "\n",
    "print \"Columns to delete:\", len(to_del_name), to_del_name\n",
    "print to_del_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Delete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if len(to_del_name) > 0:\n",
    "\n",
    "    clean_cols_ix = [i for i in range(X_train.shape[1]) if i not in to_del_ix]\n",
    "    clean_cols_name = cols[clean_cols_ix]\n",
    "    \n",
    "    print \"Clean cols:\", len(clean_cols_ix)\n",
    "\n",
    "    catCols = []\n",
    "    for i in range(len(clean_cols_name)):\n",
    "        if feat_types[clean_cols_name[i]] == 'object' \\\n",
    "            or cols[i] in (\"RONCA\",\"SUE_REP\",\"CRISIS_ASF\",\"NICTURIA\",\"APNEAS_PRES\",\"CEFALEA\",\n",
    "                    \"SOMN_DIURN\",\"DESPERTAR_NOCT\",\"TRAST_MEM\",\"TRAST_CONC\",\"IRR_APAT_DEPR\"):\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "            \n",
    "    catCols = np.array(catCols)\n",
    "    cols = clean_cols_name.tolist()\n",
    "    cols.append([\"LABEL\"])    \n",
    "    reducedCols = cols[:-1]\n",
    "    \n",
    "    X_train = X_train[:, clean_cols_ix]\n",
    "    X_test = X_test[:, clean_cols_ix]\n",
    "    X_filt = X_filt[:,clean_cols_ix]\n",
    "    \n",
    "    print \"Cat cols:\", np.sum(catCols==1)\n",
    "    print \"Num cols:\", np.sum(catCols==0)\n",
    "    print len(reducedCols)\n",
    "    print X_train.shape\n",
    "    print X_test.shape\n",
    "    print X_filt.shape\n",
    "    print cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Outlier filtering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.187628211208 9.74966275351 9.93729096472\n",
      "Ouliers min ratio: 10.2187332815 9.4682204367\n",
      "Num outliers detected: 1\n",
      "Num outliers detected: 2\n",
      "[('MY02', 10.056445015986377), ('MY26', 9.9323251930368119), ('MY28', 9.8226505975830047), ('MY37', 10.009306444770054), ('MY33', 9.8559365243865784), ('MY24', 10.06015584937227), ('MY12', 9.5955126410019957), ('MY50', 9.9242615267190466), ('MY42', 9.6932723135795058), ('MY45', 9.670967204896959), ('MY29', 10.092544597354381), ('MY32', 9.7452672219234486), ('MY57', 9.8496239597285626), ('MY56', 9.8678628844699787), ('MY46', 9.8472200465759485), ('MY53', 9.9389462219459528), ('MY54', 9.602810325960931), ('MY27', 9.8423222925064415), ('MY19', 10.097004589946019), ('MY21', 9.8582194532184513), ('MY61', 9.837804354081781), ('MY14', 9.8349152659842716), ('MY55', 10.53251673271707), ('MY06', 9.7848204828974854), ('MY51', 9.7628493482716863), ('MY05', 9.9750763193102987), ('MY18', 9.6988707037426956), ('MY59', 9.7958130009393347), ('MY01', 9.8562359688120313), ('MY22', 9.6076819645058169), ('MY20', 9.7092332982851204), ('MY47', 9.8851609455632907), ('MY17', 10.013606640364403), ('MY41', 9.9008538624482547), ('MY03', 9.6948201945311787), ('MY34', 10.014174392198187), ('MY09', 9.4680740308174478), ('MY36', 9.8348999964242001), ('MY44', 9.79902035238403), ('MY35', 9.2760904873431613), ('MY15', 10.032976162286767), ('MY48', 9.7990756918871913)]\n",
      "Patients outliers above: ['MY55']\n",
      "Patients outliers below: ['MY09' 'MY35']\n"
     ]
    }
   ],
   "source": [
    "outFilter = OutlierFiltering(cols,all_codes,norm=\"l2\")\n",
    "outFilter.fit(X_filt.astype(float), y_all)\n",
    "toDel =  np.hstack((outFilter.codesToDel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\n",
      "['MY55']\n",
      "[7, 10]\n",
      "['MY09' 'MY35']\n",
      "[22  7 10]\n",
      "ix_new: [1]\n",
      "29\n",
      "Train: (29, 68)\n",
      "Test: (13, 68)\n",
      "['MY02' 'MY26' 'MY28' 'MY37' 'MY33' 'MY24' 'MY12' 'MY50' 'MY42' 'MY45'\n",
      " 'MY29' 'MY32' 'MY57' 'MY56' 'MY46' 'MY53' 'MY54' 'MY27' 'MY19' 'MY21'\n",
      " 'MY61' 'MY14' 'MY06' 'MY51' 'MY05' 'MY18' 'MY59' 'MY01' 'MY20']\n",
      "['MY22' 'MY47' 'MY17' 'MY41' 'MY03' 'MY34' 'MY09' 'MY36' 'MY44' 'MY35'\n",
      " 'MY15' 'MY48' 'MY55']\n"
     ]
    }
   ],
   "source": [
    "#Remove outliers from train and add them to test\n",
    "\n",
    "ix_tr = [i for i in range(len(tr_codes)) for d in toDel if tr_codes[i] == d]\n",
    "print ix_tr\n",
    "print tr_codes[ix_tr]\n",
    "\n",
    "ix_ts = [i for i in range(len(ts_codes)) for d in toDel if ts_codes[i] == d]\n",
    "print ix_ts\n",
    "print ts_codes[ix_ts]\n",
    "\n",
    "ix_all = np.hstack((ix_tr,ix_ts))\n",
    "print ix_all\n",
    "\n",
    "#Change anomal row from train to test (if not anomaly and same label type (to assure stratification))\n",
    "ix_news = []\n",
    "for i in range(len(ix_tr)):\n",
    "    for j in range(len(ts_codes)):\n",
    "        if (j not in ix_ts) and (j not in ix_news) and y_train[ix_tr[i]] == y_test[j]:\n",
    "            ix_news.append(j)\n",
    "            break\n",
    "        \n",
    "print \"ix_new:\", ix_news\n",
    "print len(tr_codes)\n",
    "\n",
    "\n",
    "#Remove outliers individuals from train\n",
    "X_test = np.vstack((X_test, X_train[ix_tr,:]))\n",
    "y_test = np.hstack((y_test, y_train[ix_tr]))\n",
    "ts_codes = np.hstack((ts_codes,tr_codes[ix_tr]))\n",
    "\n",
    "X_train = np.delete(X_train, (ix_tr), axis=0)\n",
    "y_train  = np.delete(y_train, (ix_tr), axis=0)\n",
    "tr_codes = np.delete(tr_codes,(ix_tr), axis=0)\n",
    "    \n",
    "trick = True\n",
    "if trick:    \n",
    "    #Add healthy individual from test to train  \n",
    "    for ix_new in ix_news:        \n",
    "        X_train = np.vstack((X_train, X_test[ix_new,:]))\n",
    "        y_train = np.hstack((y_train, y_test[ix_new]))\n",
    "        tr_codes = np.hstack((tr_codes,ts_codes[ix_new]))\n",
    "\n",
    "        X_test = np.delete(X_test, (ix_new), axis=0)\n",
    "        y_test  = np.delete(y_test, (ix_new), axis=0)\n",
    "        ts_codes = np.delete(ts_codes,(ix_new), axis=0)\n",
    "    \n",
    "\n",
    "print \"Train:\", X_train.shape\n",
    "print \"Test:\", X_test.shape\n",
    "print tr_codes\n",
    "print ts_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### PRE-Select features (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "11\n",
      "Clean cols: 11\n",
      "Cat cols: 8\n",
      "Num cols: 3\n",
      "11\n",
      "12\n",
      "11\n",
      "(29, 11)\n",
      "(13, 11)\n",
      "(42, 11)\n"
     ]
    }
   ],
   "source": [
    "#dfFeat = joblib.load(os.path.join('resources','rfecv_cl0_short_3m_1m_monit_14012016.pkl'))\n",
    "#print np.sum(dfFeat.get_support() == 1)\n",
    "\n",
    "#selCols = ['JUBILADO', 'FUMA_PAQ_ANY', 'HTA' ,'CARDIOPATIA', 'DIABETES', 'DISLIPEMIA',\n",
    "# 'DESPERTAR_NOCT', 'INSOMNIO', 'DOLOR_PIER_MOVER', 'SEN_SUE_REAL_DORM',\n",
    "# 'DISM_DESEO_SEX', 'BBLOQ', 'INSULINA', 'BDZ', 'OTROS_HIPOT', 'IMC', 'CIRC_ABD',\n",
    "# 'AL_IRR_CUT_MAS_3', 'BOCA_SECA_3', 'EPWORTH_3', 'VISUAL EUROQOL_3']\n",
    "#selCols = ['BOCA_SECA_3', 'INSOMNIO', 'DOLOR_PIER_MOVER', 'FUMA', 'EUROQOL_1', 'VISUAL']\n",
    "\n",
    "selCols = ['INSOMNIO', 'CIRC_ABD', 'BBLOQ', 'ANSIEDAD', 'CARDIOPATIA', 'DIABETES', 'DISLIPEMIA', \n",
    "           'OTROS_PSICO', 'TIEMP_SUENO', 'PESO','DOLOR_PIER_MOVER', 'MOV_BRUSC_INV_DUERME', 'MEDIA_TAD', \n",
    "           'FUMA_EX_ANYS', 'ALCOHOL_GR_DIA', 'ANTIAGREG', 'OTROS_HIPOT', 'ENF_NEUR', 'CEFALEA', 'FUMA']\n",
    "\n",
    "print len(selCols)\n",
    "\n",
    "filtSelCols = [c for i,c in enumerate(cols) if c in selCols] \n",
    "print len(filtSelCols)\n",
    "\n",
    "if len(selCols) > 0:\n",
    "\n",
    "    clean_cols_ix = [i for i,c in enumerate(cols) if c in filtSelCols]    \n",
    "    clean_cols_name = filtSelCols\n",
    "    \n",
    "    print \"Clean cols:\", len(clean_cols_ix)\n",
    "\n",
    "    catCols = []\n",
    "    for i in range(len(clean_cols_name)):\n",
    "        if feat_types[clean_cols_name[i]] == 'object' \\\n",
    "            or clean_cols_name[i] in (\"RONCA\",\"SUE_REP\",\"CRISIS_ASF\",\"NICTURIA\",\"APNEAS_PRES\",\"CEFALEA\",\n",
    "                    \"SOMN_DIURN\",\"DESPERTAR_NOCT\",\"TRAST_MEM\",\"TRAST_CONC\",\"IRR_APAT_DEPR\"):\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "            \n",
    "    catCols = np.array(catCols)\n",
    "    cols = clean_cols_name\n",
    "    cols.append([\"LABEL\"])\n",
    "    reducedCols = cols[:-1]\n",
    "    \n",
    "    X_train = X_train[:, clean_cols_ix]\n",
    "    X_test = X_test[:, clean_cols_ix]\n",
    "    X_filt = X_filt[:,clean_cols_ix]\n",
    "    \n",
    "    print \"Cat cols:\", np.sum(catCols==1)\n",
    "    print \"Num cols:\", np.sum(catCols==0)\n",
    "    print len(catCols)\n",
    "    print len(cols)\n",
    "    print len(reducedCols)\n",
    "    print X_train.shape\n",
    "    print X_test.shape\n",
    "    print X_filt.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Config pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fs_methods = [\"none\"] \n",
    "#\"rfe_logReg_fs\",\"rfe_rf_fs\",\"mi_fs\",\"iscore_fs\", \"discrete_fs\", \"continuous_fs\",\n",
    "#\"none\",\"combine_fs\",\"fusion_fs\",sfs_fs,lasso_fs\n",
    "cls_method = \"nn\"\n",
    "sm_method = \"none\" # sm_smote, none\n",
    "sm_type = \"after\"# before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ds = [0,1,2]\n",
    "suffixes = ['_no_out','']\n",
    "metrics = [\"precision_weighted\",\"f1_weighted\"]\n",
    "all_fs_methods = [\"no_fs\", \"extraTrees_fs\", \"rfe_fs\",\n",
    "                  \"lasso_fs\", \"ftest_fs\",\"fpr_fs\", \"fdr_fs\",\n",
    "                  \"mi_fs\",\"randLasso_fs\"]\n",
    "samplers_types = [\"none\", \"before\", \"after\"]\n",
    "samplers_methods = [\"none\",\"sm_smote\", \"sm_ada\"]\n",
    "all_cls_methods = [\"svmRBF\",\"logReg\",\"knn\",\"rf\",\"nb\",\"vot\",\"nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    params.update({cls_method + '__SVC__C': [0.01,0.1,0.5,1,3,5,7,10,30,100], \\n                   cls_method + '__gamma' : [0.0001,0.001,0.01, 0.1,1,3],\\n                   cls_method + '__class_weight': [None, 'balanced']})\\n    \\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basePipeline = Pipeline([\n",
    "        (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "        (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "    ])\n",
    "\n",
    "params = {}\n",
    "pipeline = []\n",
    "pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "#Feat selection\n",
    "for fs_method in fs_methods:\n",
    "    \n",
    "    if fs_method == \"sfs_fs\":\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        rf = RandomForestClassifier(class_weight=None,n_estimators=50,criterion='gini',random_state=33)\n",
    "        sfs = SFS(knn, \n",
    "                  k_features=6, \n",
    "                  forward=True, \n",
    "                  floating=True, \n",
    "                  scoring='f1_weighted',\n",
    "                  verbose=0,\n",
    "                  n_jobs = -1,\n",
    "                  cv=3)\n",
    "        pipe.steps.append((fs_method,sfs))\n",
    "\n",
    "    if fs_method == \"combine_mix_fs\":\n",
    "        pipe.steps.append((fs_method, \\\n",
    "            FeatureUnion(transformer_list=[\n",
    "            ('disc_pipe', Pipeline([\\\n",
    "            (\"disc\", DiscreteFilter(catCols)),\\\n",
    "            #(\"disc_sfm\", SelectFromModel(RandomForestClassifier(n_estimators=50,random_state=33)))\\\n",
    "            (\"disc_sfm\", SelectKBest(feature_selection.chi2))\\\n",
    "            ])),\\\n",
    "            ('cont_pipe', Pipeline([\\\n",
    "            (\"cont\", ContinuousFilter(catCols)),\\\n",
    "            #(\"cont_sfm\", SelectFromModel(LogisticRegression(penalty=\"l1\", dual=False, random_state=42)))\\\n",
    "            (\"cont_sfm\", SelectKBest(feature_selection.mutual_info_classif))\\\n",
    "            ]))\\\n",
    "            ])\\\n",
    "            ))\n",
    "        params.update({fs_method + \"__disc_pipe__disc_sfm__k\":[5,7,10]})  \n",
    "        params.update({fs_method + \"__cont_pipe__cont_sfm__k\":[2,5,7]})     \n",
    "        \n",
    "    if fs_method == \"combine_fs\":\n",
    "        pipe.steps.insert(1,(fs_method, UnivCombineFilter(catCols,np.array(reducedCols))))\n",
    "        params.update({fs_method + '__percentile':[10,15,20,25,30,50]})\n",
    "        \n",
    "    if fs_method == \"combine_old_fs\":\n",
    "        pipe.steps.insert(1,(fs_method, FeatureUnion([(\"disc\", DiscreteFS(catCols,oneMinFeat=False)), \n",
    "                                                    (\"cont\", ContinuousFS(catCols,oneMinFeat=False))])))\n",
    "    if fs_method == \"discrete_fs\":\n",
    "        pipe.steps.append((fs_method, DiscreteFS(catCols)))\n",
    "        \n",
    "    if fs_method == \"continuous_fs\":\n",
    "        pipe.steps.append((fs_method, ContinuousFS(catCols)))\n",
    "        \n",
    "    if fs_method == \"rfe_rf_fs\":\n",
    "        pipe.steps.append((fs_method, RFE(estimator=RandomForestClassifier(class_weight=None,\n",
    "                                                                           n_estimators=50,\n",
    "                                                                           criterion='gini',\n",
    "                                                                           random_state=33))))\n",
    "        params.update({fs_method + '__step':[0.1]})\n",
    "        params.update({fs_method + '__n_features_to_select': #[10,13]}) #7,9,10,11,\n",
    "                                        [int(len(reducedCols)*0.2), \n",
    "                                        int(len(reducedCols)*0.4), \n",
    "                                        int(len(reducedCols)*0.6), \n",
    "                                        int(len(reducedCols)*0.8)]})        \n",
    "\n",
    "    if fs_method == \"rfe_logReg_fs\":\n",
    "\n",
    "        pipe.steps.append((fs_method, RFE(estimator=LogisticRegression(penalty=\"l1\",random_state=33))))\n",
    "        #params.update({fs_method + '__step':[1]})\n",
    "        #params.update({fs_method + '__n_features_to_select': [7,9,10,11,13]})\n",
    "        params.update({fs_method + '__step':[0.1]})\n",
    "        params.update({fs_method + '__n_features_to_select':[int(len(reducedCols)*0.2), \n",
    "                                        int(len(reducedCols)*0.4), \n",
    "                                        int(len(reducedCols)*0.6), \n",
    "                                        int(len(reducedCols)*0.8)]})\n",
    "        #params.update({fs_method + '__estimator__C':[0.001,0.001,0.01,0.1,1]})\n",
    "        #params.update({fs_method + '__estimator__class_weight':[None,'balanced']})\n",
    "                          \n",
    "                           \n",
    "    if fs_method == \"ftest_fs\":\n",
    "        pipe.steps.append((fs_method, SelectKBest(feature_selection.f_classif)))\n",
    "        params.update({fs_method + '__k':[7,10,15,20]})\n",
    "\n",
    "    if fs_method == \"mi_fs\":\n",
    "        pipe.steps.append((fs_method, SelectKBest(feature_selection.mutual_info_classif)))\n",
    "        params.update({fs_method + '__k':[7,10,15,20]})\n",
    "\n",
    "    if fs_method == \"iscore_fs\":\n",
    "        pipe.steps.append((fs_method, SelectKBest(i_score)))\n",
    "        params.update({fs_method + '__k':[7,10,15,20]})    \n",
    "        \n",
    "    if fs_method == \"fdr_fs\":\n",
    "        pipe.steps.append((fs_method, SelectFdr(f_classif)))\n",
    "\n",
    "    if fs_method == \"fpr_fs\":\n",
    "        pipe.steps.append((fs_method, SelectFpr(f_classif)))\n",
    "\n",
    "    if fs_method == 'extraTrees_fs':\n",
    "        pipe.steps.append((fs_method, SelectFromModel(\n",
    "                    RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=24))))\n",
    "\n",
    "    if fs_method == 'lasso_fs':\n",
    "        pipe.steps.append((fs_method, SelectFromModel(\n",
    "                    LogisticRegression(n_jobs=-1, penalty=\"l1\", dual=False, random_state=42))))\n",
    "        params.update({fs_method + '__estimator__C': [0.5,1,3]})\n",
    "\n",
    "    if fs_method == 'randLasso_fs':\n",
    "        pipe.steps.append((fs_method, \n",
    "                    RandomizedLogisticRegression(sample_fraction=1, fit_intercept =False, \n",
    "                                                 n_jobs=-1, random_state=42)))\n",
    "\n",
    "\n",
    "\n",
    "#Add classifiers\n",
    "if cls_method == \"knn\":\n",
    "    pipe.steps.append((cls_method, KNeighborsClassifier()))\n",
    "    params.update({'knn__n_neighbors':[1,3,5,7,9,11], 'knn__weights':['uniform', 'distance']})\n",
    "\n",
    "if cls_method == \"nb\":                              \n",
    "    pipe.steps.append((cls_method, GaussianNB()))\n",
    "\n",
    "if cls_method == \"logReg\":\n",
    "    pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "    params.update({'logReg__C': [0.00001,0.0001,0.001,0.01,0.1,0.5,1,5,10,50,100]})\n",
    "    params.update({'logReg__class_weight': [None, 'balanced']})\n",
    "    params.update({'logReg__penalty': ['l1', 'l2']})\n",
    "    \n",
    "if cls_method == \"svmLin\":\n",
    "    pipe.steps.append((cls_method, SVC(kernel='linear', probability=True,random_state=42)))\n",
    "    params.update({cls_method + '__C': [0.00001,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,15,30,50,100]})\n",
    "    params.update({cls_method + '__class_weight': [None, 'balanced']})\n",
    "     \n",
    "if cls_method == \"svmRBF\":\n",
    "    pipe.steps.append((cls_method, SVC(random_state=42,probability=True)))\n",
    "    params.update({'svmRBF__C': [0.001,0.01,0.1,0.5,1,5,10,30,100,150], \n",
    "     'svmRBF__gamma' : [0.00001,0.0001,0.001,0.01, 0.1,1]})\n",
    "    params.update({'svmRBF__class_weight': [None, 'balanced']})\n",
    "\n",
    "if cls_method == \"rf\":\n",
    "    pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,random_state=42)))\n",
    "    params.update({'rf__n_estimators': [100,200,500,750,1000], 'rf__criterion': ['entropy','gini'],\n",
    "                 'rf__max_depth' : [None,2,4,5]})\n",
    "    params.update({'rf__class_weight': [None, 'balanced']})\n",
    "\n",
    "if cls_method == \"nn\":\n",
    "    pipe.steps.append((cls_method, MLPClassifier(\n",
    "                activation='logistic',\n",
    "                solver='lbfgs', \n",
    "                hidden_layer_sizes=(5, 2), \n",
    "                random_state=13)))\n",
    "    params.update({\n",
    "            'nn__alpha': [1e-5,0.00001,0.0001,0.001,0.01,0.1,1,3,5,10],\n",
    "            'nn__hidden_layer_sizes':[(30,),(50,),(70,),(150,),\n",
    "                                      (30,30),(50,50),(70,70),\n",
    "                                      (30,30,30),(50,50,50),(70,70,70)\n",
    "                                     ]\n",
    "                  })\n",
    "\n",
    "if cls_method == \"ada\":\n",
    "    pipe.steps.append((cls_method, AdaBoostClassifier(random_state=42)))\n",
    "    params.update({'ada__n_estimators': [5,10,50,100,200],\n",
    "                 'ada__learning_rate' : [0.01,0.1,0.5,1.0],\n",
    "                 'ada__base_estimator': [KNeighborsClassifier(n_neighbors=3)]})\n",
    "\n",
    "\n",
    "    \n",
    "if cls_method == \"vot\":\n",
    "    pipe.steps.append((cls_method, VotingClassifier(voting='soft', estimators=\n",
    "            [\n",
    "                ('lr', LogisticRegression(random_state=42,penalty=\"l1\")),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
    "                ('svm', SVC(kernel=\"rbf\",random_state=42,probability=True)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=10, n_jobs=-1,random_state=42)),\n",
    "                #('ada', AdaBoostClassifier(random_state=42)),\n",
    "                ('nb', GaussianNB()),\n",
    "            ])))\n",
    "    \n",
    "if cls_method == \"ensemble\":\n",
    "    clf1 = KNeighborsClassifier(n_neighbors=9)\n",
    "    clf2 = RandomForestClassifier(n_estimators=200, criterion=\"gini\",class_weight=None,\n",
    "                                  max_depth=4,random_state=1)\n",
    "    clf3 = SVC(C=10,gamma=0.001,class_weight=None, probability=True, kernel=\"rbf\",random_state=1)\n",
    "    clf4 = LogisticRegression(class_weight=None,C=0.5,penalty=\"l1\",random_state=1)\n",
    "    clf5 = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    meta = RandomForestClassifier(n_estimators=100)\n",
    "    \n",
    "    sclf = StackingClassifier(classifiers=[cl1,clf2, clf3, clf4,cl5],\n",
    "                              meta_classifier=meta, use_probas=True, average_probas=True)\n",
    "    pipe.steps.append((cls_method, sclf))\n",
    "    \n",
    "\"\"\"\n",
    "    params.update({cls_method + '__SVC__C': [0.01,0.1,0.5,1,3,5,7,10,30,100], \n",
    "                   cls_method + '__gamma' : [0.0001,0.001,0.01, 0.1,1,3],\n",
    "                   cls_method + '__class_weight': [None, 'balanced']})\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fs</th>\n",
       "      <th>cls</th>\n",
       "      <th>smp</th>\n",
       "      <th>pipe</th>\n",
       "      <th>pipe_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>nn</td>\n",
       "      <td>none</td>\n",
       "      <td>Pipeline(steps=[('Imputer', TypeFeatImputer(al...</td>\n",
       "      <td>{u'nn__hidden_layer_sizes': [(30,), (50,), (70...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fs cls   smp                                               pipe  \\\n",
       "0  [none]  nn  none  Pipeline(steps=[('Imputer', TypeFeatImputer(al...   \n",
       "\n",
       "                                         pipe_params  \n",
       "0  {u'nn__hidden_layer_sizes': [(30,), (50,), (70...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplers = [\n",
    "    (\"none\", None),\n",
    "    (\"sm_smote\", SMOTE(ratio='auto', random_state=32)),\n",
    "    (\"sm_ada\", ADASYN(random_state=32))]\n",
    "\n",
    "pipeline = []\n",
    "\n",
    "#Create pipeline object (avoiding certain tests)\n",
    "#if not (cls_method == \"rf\" and fs_method == 'extraTrees_fs'):    \n",
    "        \n",
    "for i, sampler_method in enumerate(samplers_methods):\n",
    "    for sampler_type in samplers_types:            \n",
    "\n",
    "        #Cast pipeline to imb_pipeline\n",
    "        pipe_imb = make_pipeline(*[p[1] for p in pipe.steps])\n",
    "        stps = len(pipe_imb.steps)        \n",
    "        for s in range(stps):\n",
    "            pipe_imb.steps.remove(pipe_imb.steps[0])\n",
    "        for s in range(stps):\n",
    "            pipe_imb.steps.append(pipe.steps[s])\n",
    "\n",
    "        #Append samplings\n",
    "        if str(sampler_method).lower() == sm_method.lower() and sampler_type.lower() == sm_type.lower() \\\n",
    "                and sm_method.lower() == \"none\":\n",
    "            pipeline.append([[fs_method for fs_method in fs_methods], cls_method, sm_method, pipe_imb, params])            \n",
    "\n",
    "        if sampler_method == sm_method and sampler_type == sm_type and sampler_type == \"after\" \\\n",
    "            and sm_method.lower() != \"none\":\n",
    "                \n",
    "            pipe_imb.steps.insert(stps - 1, samplers[i])\n",
    "            pipeline.append([[fs_method for fs_method in fs_methods], cls_method, sm_method + \"_\" + sm_type, pipe_imb, params])\n",
    "            if sm_method == \"sm_smote\":\n",
    "                params.update({sm_method + \"__k_neighbors\":[3,4,5],\n",
    "                               sm_method + \"__kind\":['regular','svm']})\n",
    "            else:\n",
    "                params.update({sm_method + \"__n_neighbors\":[3,4,5],\n",
    "                               sm_method + \"__kind\":['regular','svm']})\n",
    "\n",
    "        if sampler_method == sm_method and sampler_type == sm_type and sampler_type == \"before\" \\\n",
    "            and sm_method.lower() != \"none\":\n",
    "            \n",
    "            pipe_imb.steps.insert(3, samplers[i])                \n",
    "            pipeline.append([[fs_method for fs_method in fs_methods], cls_method, sm_method + \"_\" + sm_type, pipe_imb, params])\n",
    "            if sm_method == \"sm_smote\":\n",
    "                params.update({sm_method + \"__k_neighbors\":[3,4,5],\n",
    "                               sm_method + \"__kind\":['regular','svm']})\n",
    "            else:\n",
    "                params.update({sm_method + \"__n_neighbors\":[3,4,5],\n",
    "                               sm_method + \"__kind\":['regular','svm']})\n",
    "            \n",
    "            \n",
    "pipeline = pd.DataFrame(pipeline, columns=[\"fs\",\"cls\",\"smp\",\"pipe\",\"pipe_params\"])\n",
    "pipeline.sort_values(\"fs\", inplace=True)\n",
    "pipeline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Config experiment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Imputer', TypeFeatImputer(allNameCols=array(['GENERO', 'ACTIVO', ..., 'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object),\n",
      "        dataCatCols=array([1, 1, ..., 0, 0]))), ('Scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('Variance', VarianceThreshold(threshold=0.0)), ('nn', MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=13, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "num_exp = 0\n",
    "verbose = False\n",
    "metric = \"f1_weighted\"\n",
    "cv_thr = 0.3\n",
    "\n",
    "print pipeline.iloc[0,3].steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL TRAIN: (29, 70)\n",
      "TRAIN: [0's: 12 1's: 17 ]\n",
      "ALL TEST: (13, 70)\n",
      "TEST: [0's: 6 1's: 7 ]\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "8 12\n",
      "4 5\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.5s\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/aegle/miniconda2/envs/myosa/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV INNER selected params [(70, 70), 0.01]\n",
      "CV f1_weighted score: 0.759090909091\n",
      "CV f1 score: 0.759  (+/-0.151)\n",
      "\n",
      "CV OUTER selected params [(70, 70), 0.01]\n",
      "CV f1 score: 0.827  (+/-0.081)\n",
      "Selected params (bests from CV) [(70, 70), 0.01]\n",
      "\n",
      "TR Prec score: 1.0\n",
      "TR F1 score: 1.0\n",
      "\n",
      "Test f1: 0.844\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86         6\n",
      "          1       1.00      0.71      0.83         7\n",
      "\n",
      "avg / total       0.88      0.85      0.84        13\n",
      "\n",
      "[[6 0]\n",
      " [2 5]]\n",
      "Total time: 22.1901941299\n"
     ]
    }
   ],
   "source": [
    "print \"ALL TRAIN:\", X_train.shape\n",
    "print \"TRAIN:\", \"[0's:\", np.sum(y_train==0), \"1's:\", np.sum(y_train==1), \"]\"\n",
    "print \"ALL TEST:\", X_test.shape\n",
    "print \"TEST:\", \"[0's:\", np.sum(y_test==0), \"1's:\", np.sum(y_test==1), \"]\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Run experiment\n",
    "    start = time.time()\n",
    "\n",
    "    #Prepare pipe_cls      \n",
    "    pipeline_cls = pipeline[\"pipe\"].iloc[num_exp]\n",
    "    pipeline_params = pipeline[\"pipe_params\"].iloc[num_exp]\n",
    "\n",
    "    if verbose:\n",
    "        print \"\\n\",pipeline_cls.steps\n",
    "\n",
    "    \n",
    "    #Prepare cv\n",
    "    cv_inner = StratifiedShuffleSplit(y_train, n_iter=10, test_size=cv_thr,random_state=24)\n",
    "    cv_outer = StratifiedShuffleSplit(y_train, n_iter=10, test_size=cv_thr,random_state=42)\n",
    "\n",
    "    #for cv in cv_inner:\n",
    "    #    print np.sum(y_train[cv[0]] == 0), np.sum(y_train[cv[0]] == 1)\n",
    "    #    print np.sum(y_train[cv[1]] == 0), np.sum(y_train[cv[1]] == 1)\n",
    "    \n",
    "    #Fit pipeline with CV                        \n",
    "    grid_pipeline = GridSearchCV(pipeline_cls, param_grid=pipeline_params, verbose=1, \n",
    "                                 n_jobs=-1, cv=cv_inner, scoring= metric, error_score = 0,\n",
    "                                 refit=True) \n",
    "    grid_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Compute pipeline evaluation with CV\n",
    "    print \"\\nCV INNER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "    \n",
    "    cv_inner_f1 = cross_validation.cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                             cv=cv_inner, scoring='f1_weighted', n_jobs=-1)\n",
    "    \n",
    "    print \"CV {} score: {}\".format(metric, grid_pipeline.best_score_)\n",
    "    print \"CV f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_inner_f1), np.std(cv_inner_f1))\n",
    "    \n",
    "    cv_f1 = cross_validation.cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                             cv=cv_outer, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "    print \"\\nCV OUTER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "    print \"CV f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_f1), np.std(cv_f1))\n",
    "    print \"Selected params (bests from CV) {}\".format(grid_pipeline.best_params_.values())\n",
    "\n",
    "    # Computel Train score (with best CV params)\n",
    "    y_pred = grid_pipeline.best_estimator_.predict(X_train)                \n",
    "    train_scores = precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "    train_f1_scores = f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "    \n",
    "    print \"\\nTR Prec score:\", train_scores\n",
    "    print \"TR F1 score:\", train_f1_scores\n",
    "\n",
    "    #Compute test score\n",
    "    y_pred = grid_pipeline.best_estimator_.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "    print \"\\nTest f1: %0.3f\" % (test_f1)\n",
    "    print \"with following performance in test:\"\n",
    "    print classification_report(y_test, y_pred)\n",
    "    print confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Total time:\", end - start\n",
    "\n",
    "except Exception, err:\n",
    "    print \"Error on the experiment\", err    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Imputer': TypeFeatImputer(allNameCols=array(['GENERO', 'ACTIVO', ..., 'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object),\n",
      "        dataCatCols=array([1, 1, ..., 0, 0])), 'nn__learning_rate_init': 0.001, 'Scaler__copy': True, 'nn__alpha': 0.01, 'Imputer__dataCatCols': array([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0]), 'nn__shuffle': True, 'nn__nesterovs_momentum': True, 'nn__early_stopping': False, 'nn__hidden_layer_sizes': (70, 70), 'nn': MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(70, 70), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=13, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False), 'nn__max_iter': 200, 'nn__verbose': False, 'nn__batch_size': 'auto', 'nn__power_t': 0.5, 'nn__tol': 0.0001, 'Scaler__with_mean': True, 'nn__beta_2': 0.999, 'nn__beta_1': 0.9, 'nn__warm_start': False, 'Imputer__allNameCols': array(['GENERO', 'ACTIVO', 'JUBILADO', 'EDAD', 'FUMA', 'FUMA_PAQ_ANY',\n",
      "       'FUMA_EX_ANYS', 'ALCOHOL', 'ALCOHOL_GR_DIA', 'CAFES_DIA',\n",
      "       'DEPRESION', 'ANSIEDAD', 'HTA', 'CARDIOPATIA', 'ENF_RESP',\n",
      "       'DIABETES', 'OBESIDAD', 'DISLIPEMIA', 'OBSTR_NASAL_CRON', 'RONCA',\n",
      "       'SUE_REP', 'CRISIS_ASF', 'NICTURIA', 'APNEAS_PRES', 'CEFALEA',\n",
      "       'SOMN_DIURN', 'DESPERTAR_NOCT', 'TRAST_CONC', 'IRR_APAT_DEPR',\n",
      "       'INSOMNIO', 'ACT_MOTR_NOCT', 'SEN_SUE_REAL_DORM', 'SUE_INT_PIROSIS',\n",
      "       'DISM_DESEO_SEX', 'IECAS', 'DIURETICOS', 'ANTIAGREG', 'ANTIACID',\n",
      "       'HIPOLIPEM', 'BDZ', 'TIEMP_SUENO', 'IAH', 'TIEMPOSAT<90%',\n",
      "       'IND_DESAT', 'TALLA', 'IMC', 'CIRC_CUELLO', 'MEDIA_TAS',\n",
      "       'MEDIA_TAD', 'Sat O2', 'PRESS_CPAP', 'EPWORTH', 'EUROQOL', 'VISUAL',\n",
      "       'HORAS_USO_TOT_1', 'HORAS_USO_MED_NOCHE_1', 'AL_IRR_CUT_MASC_1',\n",
      "       'BOCA_SECA_1', 'MEDIA_TAS_1', 'MEDIA_TAD_1', 'EPWORTH_1',\n",
      "       'EUROQOL_1', 'VISUAL_EUROQOL_1', 'HORAS_USO_MED_NOCHE_3',\n",
      "       'BOCA_SECA_3', 'MEDIA TAS_3', 'MEDIA TAD_3', 'EPWORTH_3',\n",
      "       'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object), 'nn__learning_rate': 'constant', 'nn__validation_fraction': 0.1, 'nn__momentum': 0.9, 'Variance': VarianceThreshold(threshold=0.0), 'nn__epsilon': 1e-08, 'Scaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'nn__random_state': 13, 'Scaler__with_std': True, 'Variance__threshold': 0.0, 'steps': [('Imputer', TypeFeatImputer(allNameCols=array(['GENERO', 'ACTIVO', ..., 'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object),\n",
      "        dataCatCols=array([1, 1, ..., 0, 0]))), ('Scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('Variance', VarianceThreshold(threshold=0.0)), ('nn', MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(70, 70), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=13, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False))], 'nn__solver': 'lbfgs', 'nn__activation': 'logistic'}\n"
     ]
    }
   ],
   "source": [
    "print grid_pipeline.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 0 0 1 0 1 0 0]\n",
      "['MY22' 'MY41' 'MY03' 'MY34' 'MY09' 'MY44' 'MY15' 'MY48']\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_pipeline.predict(X_test)\n",
    "print y_pred\n",
    "print ts_codes[y_pred == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Imputer': TypeFeatImputer(allNameCols=array(['GENERO', 'ACTIVO', ..., 'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object),\n",
      "        dataCatCols=array([1, 1, ..., 0, 0])), 'nn__learning_rate_init': 0.001, 'Scaler__copy': True, 'nn__alpha': 0.01, 'Imputer__dataCatCols': array([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0]), 'nn__shuffle': True, 'nn__nesterovs_momentum': True, 'nn__early_stopping': False, 'nn__hidden_layer_sizes': (70, 70), 'nn': MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(70, 70), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=13, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False), 'nn__max_iter': 200, 'nn__verbose': False, 'nn__batch_size': 'auto', 'nn__power_t': 0.5, 'nn__tol': 0.0001, 'Scaler__with_mean': True, 'nn__beta_2': 0.999, 'nn__beta_1': 0.9, 'nn__warm_start': False, 'Imputer__allNameCols': array(['GENERO', 'ACTIVO', 'JUBILADO', 'EDAD', 'FUMA', 'FUMA_PAQ_ANY',\n",
      "       'FUMA_EX_ANYS', 'ALCOHOL', 'ALCOHOL_GR_DIA', 'CAFES_DIA',\n",
      "       'DEPRESION', 'ANSIEDAD', 'HTA', 'CARDIOPATIA', 'ENF_RESP',\n",
      "       'DIABETES', 'OBESIDAD', 'DISLIPEMIA', 'OBSTR_NASAL_CRON', 'RONCA',\n",
      "       'SUE_REP', 'CRISIS_ASF', 'NICTURIA', 'APNEAS_PRES', 'CEFALEA',\n",
      "       'SOMN_DIURN', 'DESPERTAR_NOCT', 'TRAST_CONC', 'IRR_APAT_DEPR',\n",
      "       'INSOMNIO', 'ACT_MOTR_NOCT', 'SEN_SUE_REAL_DORM', 'SUE_INT_PIROSIS',\n",
      "       'DISM_DESEO_SEX', 'IECAS', 'DIURETICOS', 'ANTIAGREG', 'ANTIACID',\n",
      "       'HIPOLIPEM', 'BDZ', 'TIEMP_SUENO', 'IAH', 'TIEMPOSAT<90%',\n",
      "       'IND_DESAT', 'TALLA', 'IMC', 'CIRC_CUELLO', 'MEDIA_TAS',\n",
      "       'MEDIA_TAD', 'Sat O2', 'PRESS_CPAP', 'EPWORTH', 'EUROQOL', 'VISUAL',\n",
      "       'HORAS_USO_TOT_1', 'HORAS_USO_MED_NOCHE_1', 'AL_IRR_CUT_MASC_1',\n",
      "       'BOCA_SECA_1', 'MEDIA_TAS_1', 'MEDIA_TAD_1', 'EPWORTH_1',\n",
      "       'EUROQOL_1', 'VISUAL_EUROQOL_1', 'HORAS_USO_MED_NOCHE_3',\n",
      "       'BOCA_SECA_3', 'MEDIA TAS_3', 'MEDIA TAD_3', 'EPWORTH_3',\n",
      "       'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object), 'nn__learning_rate': 'constant', 'nn__validation_fraction': 0.1, 'nn__momentum': 0.9, 'Variance': VarianceThreshold(threshold=0.0), 'nn__epsilon': 1e-08, 'Scaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'nn__random_state': 13, 'Scaler__with_std': True, 'Variance__threshold': 0.0, 'steps': [('Imputer', TypeFeatImputer(allNameCols=array(['GENERO', 'ACTIVO', ..., 'EUROQOL_3', 'VISUAL EUROQOL_3'], dtype=object),\n",
      "        dataCatCols=array([1, 1, ..., 0, 0]))), ('Scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('Variance', VarianceThreshold(threshold=0.0)), ('nn', MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(70, 70), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=13, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False))], 'nn__solver': 'lbfgs', 'nn__activation': 'logistic'}\n",
      "\n",
      "CV INNER selected params {'nn__hidden_layer_sizes': (70, 70), 'nn__alpha': 0.01}\n",
      "CV INNER f1_weighted score: 0.759090909091\n",
      "\n",
      "TR Prec score: 1.0\n",
      "TR F1 score: 1.0\n",
      "\n",
      "Test f1: 0.844\n",
      "with following performance in test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86         6\n",
      "          1       1.00      0.71      0.83         7\n",
      "\n",
      "avg / total       0.88      0.85      0.84        13\n",
      "\n",
      "[[6 0]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "# Compute pipeline evaluation with CV\n",
    "print grid_pipeline.best_estimator_.get_params()\n",
    "print \"\\nCV INNER selected params {}\".format(grid_pipeline.best_params_)\n",
    "print \"CV INNER {} score: {}\".format(metric, grid_pipeline.best_score_)\n",
    "\n",
    "# Computel Train score (with best CV params)\n",
    "y_pred = grid_pipeline.predict(X_train)                \n",
    "train_scores = precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "train_f1_scores = f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "print \"\\nTR Prec score:\", train_scores\n",
    "print \"TR F1 score:\", train_f1_scores\n",
    "\n",
    "#Compute test score\n",
    "y_pred = grid_pipeline.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "print \"\\nTest f1: %0.3f\" % (test_f1)\n",
    "print \"with following performance in test:\"\n",
    "print classification_report(y_test, y_pred)\n",
    "print confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
